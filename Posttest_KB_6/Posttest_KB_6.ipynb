{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Posttest 6 Kecerdasan Buatan<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Nama : Sarah Syifani<h3>\n",
    "<h3>NIM : 2109106131<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from tensorflow.keras import Sequential, layers, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membaca Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winery</th>\n",
       "      <th>wine</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>price</th>\n",
       "      <th>type</th>\n",
       "      <th>body</th>\n",
       "      <th>acidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teso La Monja</td>\n",
       "      <td>Tinto</td>\n",
       "      <td>2013</td>\n",
       "      <td>4.9</td>\n",
       "      <td>58</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Toro</td>\n",
       "      <td>995.00</td>\n",
       "      <td>Toro Red</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artadi</td>\n",
       "      <td>Vina El Pison</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.9</td>\n",
       "      <td>31</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Vino de Espana</td>\n",
       "      <td>313.50</td>\n",
       "      <td>Tempranillo</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vega Sicilia</td>\n",
       "      <td>Unico</td>\n",
       "      <td>2009</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1793</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Ribera del Duero</td>\n",
       "      <td>324.95</td>\n",
       "      <td>Ribera Del Duero Red</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vega Sicilia</td>\n",
       "      <td>Unico</td>\n",
       "      <td>1999</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1705</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Ribera del Duero</td>\n",
       "      <td>692.96</td>\n",
       "      <td>Ribera Del Duero Red</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vega Sicilia</td>\n",
       "      <td>Unico</td>\n",
       "      <td>1996</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1309</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Ribera del Duero</td>\n",
       "      <td>778.06</td>\n",
       "      <td>Ribera Del Duero Red</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7495</th>\n",
       "      <td>Contino</td>\n",
       "      <td>Reserva</td>\n",
       "      <td>2016</td>\n",
       "      <td>4.2</td>\n",
       "      <td>392</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Rioja</td>\n",
       "      <td>19.98</td>\n",
       "      <td>Rioja Red</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>Conreria d'Scala Dei</td>\n",
       "      <td>Les Brugueres</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.2</td>\n",
       "      <td>390</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Priorato</td>\n",
       "      <td>16.76</td>\n",
       "      <td>Priorat Red</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>Mustiguillo</td>\n",
       "      <td>Finca Terrerazo</td>\n",
       "      <td>2017</td>\n",
       "      <td>4.2</td>\n",
       "      <td>390</td>\n",
       "      <td>Espana</td>\n",
       "      <td>El Terrerazo</td>\n",
       "      <td>24.45</td>\n",
       "      <td>Red</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>Matarromera</td>\n",
       "      <td>Gran Reserva</td>\n",
       "      <td>2011</td>\n",
       "      <td>4.2</td>\n",
       "      <td>389</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Ribera del Duero</td>\n",
       "      <td>64.50</td>\n",
       "      <td>Ribera Del Duero Red</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>Sei Solo</td>\n",
       "      <td>Preludio</td>\n",
       "      <td>2016</td>\n",
       "      <td>4.2</td>\n",
       "      <td>388</td>\n",
       "      <td>Espana</td>\n",
       "      <td>Ribera del Duero</td>\n",
       "      <td>31.63</td>\n",
       "      <td>Ribera Del Duero Red</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    winery             wine  year  rating  num_reviews  \\\n",
       "0            Teso La Monja            Tinto  2013     4.9           58   \n",
       "1                   Artadi    Vina El Pison  2018     4.9           31   \n",
       "2             Vega Sicilia            Unico  2009     4.8         1793   \n",
       "3             Vega Sicilia            Unico  1999     4.8         1705   \n",
       "4             Vega Sicilia            Unico  1996     4.8         1309   \n",
       "...                    ...              ...   ...     ...          ...   \n",
       "7495               Contino          Reserva  2016     4.2          392   \n",
       "7496  Conreria d'Scala Dei    Les Brugueres  2018     4.2          390   \n",
       "7497           Mustiguillo  Finca Terrerazo  2017     4.2          390   \n",
       "7498           Matarromera     Gran Reserva  2011     4.2          389   \n",
       "7499              Sei Solo         Preludio  2016     4.2          388   \n",
       "\n",
       "     country            region   price                  type  body  acidity  \n",
       "0     Espana              Toro  995.00              Toro Red   5.0      3.0  \n",
       "1     Espana    Vino de Espana  313.50           Tempranillo   4.0      2.0  \n",
       "2     Espana  Ribera del Duero  324.95  Ribera Del Duero Red   5.0      3.0  \n",
       "3     Espana  Ribera del Duero  692.96  Ribera Del Duero Red   5.0      3.0  \n",
       "4     Espana  Ribera del Duero  778.06  Ribera Del Duero Red   5.0      3.0  \n",
       "...      ...               ...     ...                   ...   ...      ...  \n",
       "7495  Espana             Rioja   19.98             Rioja Red   4.0      3.0  \n",
       "7496  Espana          Priorato   16.76           Priorat Red   4.0      3.0  \n",
       "7497  Espana      El Terrerazo   24.45                   Red   4.0      3.0  \n",
       "7498  Espana  Ribera del Duero   64.50  Ribera Del Duero Red   5.0      3.0  \n",
       "7499  Espana  Ribera del Duero   31.63  Ribera Del Duero Red   5.0      3.0  \n",
       "\n",
       "[7500 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"wines_SPA.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winery            0\n",
       "wine              0\n",
       "year              2\n",
       "rating            0\n",
       "num_reviews       0\n",
       "country           0\n",
       "region            0\n",
       "price             0\n",
       "type            545\n",
       "body           1169\n",
       "acidity        1169\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Mengecek apakah terdapat data null\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transforming, menggunakan simpleimputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[\"year\"] = imputer.fit_transform(df[[\"year\"]])\n",
    "df[\"type\"] = imputer.fit_transform(df[[\"type\"]])\n",
    "df[\"body\"] = imputer.fit_transform(df[[\"body\"]])\n",
    "df[\"acidity\"] = imputer.fit_transform(df[[\"acidity\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memeriksa Nilai Null pada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winery         0\n",
       "wine           0\n",
       "year           0\n",
       "rating         0\n",
       "num_reviews    0\n",
       "country        0\n",
       "region         0\n",
       "price          0\n",
       "type           0\n",
       "body           0\n",
       "acidity        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['num_reviews']]\n",
    "y = df['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buat arsitektur DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, None, 5)           10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10 (40.00 Byte)\n",
      "Trainable params: 10 (40.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=5, activation='linear', input_shape=(None,1))#activasi sesuaikan dengan permasalahan(klasifikasi atau regresi)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat callback untuk memonitoring val_loss\n",
    "Menggunakan library ModelCheckpoint untuk menyimpan hasil terbaik selama pelatihan\n",
    "Misalkan val_loss tidak menurun atau tidak membaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Model checkpoint untuk menyimpan model terbaik selama pelatihan (dengan val_loss terendah)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  1/165 [..............................] - ETA: 0s - loss: 0.0282 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/165 [=========================>....] - ETA: 0s - loss: 0.1076 - accuracy: 0.0000e+00\n",
      "Epoch 1: val_loss improved from inf to 1.38199, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.0000e+00 - val_loss: 1.3820 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 7.4189 - accuracy: 0.0000e+00 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\c2-21\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 1.38199 to 0.02478, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 6.1552 - accuracy: 0.0000e+00 - val_loss: 0.0248 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.0187 - accuracy: 0.0000e+00\n",
      "Epoch 3: val_loss improved from 0.02478 to 0.01815, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - val_loss: 0.0182 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0213 - accuracy: 0.0000e+00\n",
      "Epoch 4: val_loss improved from 0.01815 to 0.01662, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - val_loss: 0.0166 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0173 - accuracy: 0.0000e+00\n",
      "Epoch 5: val_loss improved from 0.01662 to 0.01595, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/500\n",
      "128/165 [======================>.......] - ETA: 0s - loss: 0.0167 - accuracy: 0.0000e+00\n",
      "Epoch 6: val_loss did not improve from 0.01595\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - val_loss: 0.0196 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 7: val_loss improved from 0.01595 to 0.01557, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0173 - accuracy: 0.0000e+00\n",
      "Epoch 8: val_loss improved from 0.01557 to 0.01544, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.0589 - accuracy: 0.0000e+00\n",
      "Epoch 9: val_loss improved from 0.01544 to 0.01536, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0171 - accuracy: 0.0000e+00\n",
      "Epoch 10: val_loss improved from 0.01536 to 0.01533, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.0145 - accuracy: 0.0000e+00\n",
      "Epoch 11: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0140 - accuracy: 0.0000e+00\n",
      "Epoch 12: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0156 - accuracy: 0.0000e+00\n",
      "Epoch 13: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0406 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 14: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - val_loss: 0.0303 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0527 - accuracy: 0.0000e+00\n",
      "Epoch 15: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.0000e+00 - val_loss: 0.0565 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/500\n",
      "115/165 [===================>..........] - ETA: 0s - loss: 0.0340 - accuracy: 0.0000e+00\n",
      "Epoch 16: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 0.0000e+00 - val_loss: 0.0234 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0244 - accuracy: 0.0000e+00\n",
      "Epoch 17: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.5036 - accuracy: 0.0000e+00\n",
      "Epoch 18: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.0000e+00 - val_loss: 0.0190 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0362 - accuracy: 0.0000e+00\n",
      "Epoch 19: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.0000e+00\n",
      "Epoch 20: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.0000e+00 - val_loss: 0.0219 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.0399 - accuracy: 0.0000e+00\n",
      "Epoch 21: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.2378 - accuracy: 0.0000e+00\n",
      "Epoch 22: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.2705 - accuracy: 0.0000e+00 - val_loss: 0.2064 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.5561 - accuracy: 0.0000e+00\n",
      "Epoch 23: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.5133 - accuracy: 0.0000e+00 - val_loss: 0.0241 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0218 - accuracy: 0.0000e+00\n",
      "Epoch 24: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.1347 - accuracy: 0.0000e+00\n",
      "Epoch 25: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.3460 - accuracy: 0.0000e+00\n",
      "Epoch 26: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.0000e+00 - val_loss: 0.0670 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.1253 - accuracy: 0.0000e+00\n",
      "Epoch 27: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1171 - accuracy: 0.0000e+00 - val_loss: 0.0202 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 2.1602 - accuracy: 0.0000e+00\n",
      "Epoch 28: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1.9721 - accuracy: 0.0000e+00 - val_loss: 0.1643 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 2.1002 - accuracy: 0.0000e+00\n",
      "Epoch 29: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1.7699 - accuracy: 0.0000e+00 - val_loss: 0.0196 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0181 - accuracy: 0.0000e+00\n",
      "Epoch 30: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - val_loss: 0.0232 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0509 - accuracy: 0.0000e+00\n",
      "Epoch 31: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0176 - accuracy: 0.0000e+00\n",
      "Epoch 32: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.0141 - accuracy: 0.0000e+00\n",
      "Epoch 33: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.0000e+00 - val_loss: 0.0397 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.1657 - accuracy: 0.0000e+00\n",
      "Epoch 34: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.0000e+00 - val_loss: 0.1993 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 1.2964 - accuracy: 0.0000e+00\n",
      "Epoch 35: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.1957 - accuracy: 0.0000e+00 - val_loss: 0.0166 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.0159 - accuracy: 0.0000e+00\n",
      "Epoch 36: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.0000e+00\n",
      "Epoch 37: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0175 - accuracy: 0.0000e+00\n",
      "Epoch 38: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.1057 - accuracy: 0.0000e+00\n",
      "Epoch 39: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1140 - accuracy: 0.0000e+00 - val_loss: 0.0300 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 0.2320 - accuracy: 0.0000e+00\n",
      "Epoch 40: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0449 - accuracy: 0.0000e+00\n",
      "Epoch 41: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.0000e+00 - val_loss: 0.0221 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0314 - accuracy: 0.0000e+00\n",
      "Epoch 42: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.1948 - accuracy: 0.0000e+00\n",
      "Epoch 43: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.0000e+00 - val_loss: 0.0698 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.3777 - accuracy: 0.0000e+00\n",
      "Epoch 44: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0159 - accuracy: 0.0000e+00\n",
      "Epoch 45: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0000e+00 - val_loss: 0.1163 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.0306 - accuracy: 0.0000e+00\n",
      "Epoch 46: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0220 - accuracy: 0.0000e+00\n",
      "Epoch 47: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.0000e+00 - val_loss: 0.0164 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.1285 - accuracy: 0.0000e+00\n",
      "Epoch 48: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.0000e+00 - val_loss: 0.1112 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.3673 - accuracy: 0.0000e+00\n",
      "Epoch 49: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.0000e+00 - val_loss: 0.1054 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.3662 - accuracy: 0.0000e+00\n",
      "Epoch 50: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.3996 - accuracy: 0.0000e+00 - val_loss: 5.6384 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 1.8850 - accuracy: 0.0000e+00\n",
      "Epoch 51: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 1.7819 - accuracy: 0.0000e+00 - val_loss: 0.0231 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0162 - accuracy: 0.0000e+00\n",
      "Epoch 52: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.0000e+00\n",
      "Epoch 53: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.2847 - accuracy: 0.0000e+00\n",
      "Epoch 54: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.2718 - accuracy: 0.0000e+00 - val_loss: 0.1312 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.3399 - accuracy: 0.0000e+00\n",
      "Epoch 55: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.3137 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0152 - accuracy: 0.0000e+00\n",
      "Epoch 56: val_loss did not improve from 0.01533\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0153 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0318 - accuracy: 0.0000e+00\n",
      "Epoch 57: val_loss improved from 0.01533 to 0.01527, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.1455 - accuracy: 0.0000e+00\n",
      "Epoch 58: val_loss did not improve from 0.01527\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.0000e+00 - val_loss: 0.0163 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0345 - accuracy: 0.0000e+00\n",
      "Epoch 59: val_loss improved from 0.01527 to 0.01521, saving model to best_model.h5\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0326 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0154 - accuracy: 0.0000e+00\n",
      "Epoch 60: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0185 - accuracy: 0.0000e+00\n",
      "Epoch 61: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - val_loss: 0.0175 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.8744 - accuracy: 0.0000e+00\n",
      "Epoch 62: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.0170 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0289 - accuracy: 0.0000e+00\n",
      "Epoch 63: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0271 - accuracy: 0.0000e+00 - val_loss: 0.0684 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 3.7188 - accuracy: 0.0000e+00\n",
      "Epoch 64: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 3.5596 - accuracy: 0.0000e+00 - val_loss: 0.0188 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0411 - accuracy: 0.0000e+00\n",
      "Epoch 65: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0175 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0264 - accuracy: 0.0000e+00\n",
      "Epoch 66: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0263 - accuracy: 0.0000e+00 - val_loss: 0.0178 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.0480 - accuracy: 0.0000e+00\n",
      "Epoch 67: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0477 - accuracy: 0.0000e+00 - val_loss: 0.0167 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 0.0000e+00\n",
      "Epoch 68: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0351 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.0164 - accuracy: 0.0000e+00\n",
      "Epoch 69: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - val_loss: 0.0167 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0437 - accuracy: 0.0000e+00\n",
      "Epoch 70: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.0000e+00 - val_loss: 0.0308 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 71: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - val_loss: 0.0232 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.1704 - accuracy: 0.0000e+00\n",
      "Epoch 72: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1387 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0259 - accuracy: 0.0000e+00\n",
      "Epoch 73: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.1054 - accuracy: 0.0000e+00\n",
      "Epoch 74: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.0000e+00 - val_loss: 1.0178 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.7391 - accuracy: 0.0000e+00\n",
      "Epoch 75: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.7299 - accuracy: 0.0000e+00 - val_loss: 0.0179 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0568 - accuracy: 0.0000e+00\n",
      "Epoch 76: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 77: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0227 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.2736 - accuracy: 0.0000e+00\n",
      "Epoch 78: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/500\n",
      "123/165 [=====================>........] - ETA: 0s - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 79: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.0172 - accuracy: 0.0000e+00\n",
      "Epoch 80: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0319 - accuracy: 0.0000e+00\n",
      "Epoch 81: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0518 - accuracy: 0.0000e+00\n",
      "Epoch 82: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.0000e+00 - val_loss: 0.0173 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0156 - accuracy: 0.0000e+00\n",
      "Epoch 83: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.0000e+00 - val_loss: 0.0219 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/500\n",
      "118/165 [====================>.........] - ETA: 0s - loss: 0.2623 - accuracy: 0.0000e+00\n",
      "Epoch 84: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.0176 - accuracy: 0.0000e+00\n",
      "Epoch 85: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.0138 - accuracy: 0.0000e+00\n",
      "Epoch 86: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0164 - accuracy: 0.0000e+00\n",
      "Epoch 87: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.0517 - accuracy: 0.0000e+00\n",
      "Epoch 88: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.0000e+00 - val_loss: 0.0421 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 1.2218 - accuracy: 0.0000e+00\n",
      "Epoch 89: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 1.0817 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0739 - accuracy: 0.0000e+00\n",
      "Epoch 90: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0715 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.4357 - accuracy: 0.0000e+00\n",
      "Epoch 91: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.0000e+00 - val_loss: 0.1174 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.8480 - accuracy: 0.0000e+00\n",
      "Epoch 92: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.8773 - accuracy: 0.0000e+00 - val_loss: 0.2484 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/500\n",
      "128/165 [======================>.......] - ETA: 0s - loss: 0.0340 - accuracy: 0.0000e+00\n",
      "Epoch 93: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.0000e+00 - val_loss: 0.0196 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.0258 - accuracy: 0.0000e+00\n",
      "Epoch 94: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.0156 - accuracy: 0.0000e+00\n",
      "Epoch 95: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.0000e+00\n",
      "Epoch 96: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.1187 - accuracy: 0.0000e+00\n",
      "Epoch 97: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.0000e+00 - val_loss: 0.0168 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0166 - accuracy: 0.0000e+00\n",
      "Epoch 98: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - val_loss: 0.0265 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.0806 - accuracy: 0.0000e+00\n",
      "Epoch 99: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.0196 - accuracy: 0.0000e+00\n",
      "Epoch 100: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0388 - accuracy: 0.0000e+00\n",
      "Epoch 101: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.0000e+00 - val_loss: 0.0350 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/500\n",
      "108/165 [==================>...........] - ETA: 0s - loss: 0.0164 - accuracy: 0.0000e+00\n",
      "Epoch 102: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.7789 - accuracy: 0.0000e+00\n",
      "Epoch 103: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.7703 - accuracy: 0.0000e+00 - val_loss: 0.0903 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.2956 - accuracy: 0.0000e+00\n",
      "Epoch 104: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.0000e+00\n",
      "Epoch 105: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0140 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/500\n",
      "110/165 [===================>..........] - ETA: 0s - loss: 0.1574 - accuracy: 0.0000e+00\n",
      "Epoch 106: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.0000e+00 - val_loss: 0.0260 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0668 - accuracy: 0.0000e+00\n",
      "Epoch 107: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.0000e+00 - val_loss: 0.5921 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/500\n",
      "121/165 [=====================>........] - ETA: 0s - loss: 0.1122 - accuracy: 0.0000e+00\n",
      "Epoch 108: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/500\n",
      "124/165 [=====================>........] - ETA: 0s - loss: 0.0633 - accuracy: 0.0000e+00\n",
      "Epoch 109: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.0000e+00 - val_loss: 1.8133 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/500\n",
      "118/165 [====================>.........] - ETA: 0s - loss: 4.1015 - accuracy: 0.0000e+00\n",
      "Epoch 110: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 3.0303 - accuracy: 0.0000e+00 - val_loss: 0.0176 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/500\n",
      "120/165 [====================>.........] - ETA: 0s - loss: 0.0689 - accuracy: 0.0000e+00\n",
      "Epoch 111: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.0000e+00 - val_loss: 0.0174 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/500\n",
      "114/165 [===================>..........] - ETA: 0s - loss: 0.2572 - accuracy: 0.0000e+00\n",
      "Epoch 112: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.0000e+00 - val_loss: 0.0697 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/500\n",
      "111/165 [===================>..........] - ETA: 0s - loss: 0.0202 - accuracy: 0.0000e+00\n",
      "Epoch 113: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/500\n",
      "125/165 [=====================>........] - ETA: 0s - loss: 0.0189 - accuracy: 0.0000e+00\n",
      "Epoch 114: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - val_loss: 0.0335 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/500\n",
      "111/165 [===================>..........] - ETA: 0s - loss: 0.0688 - accuracy: 0.0000e+00\n",
      "Epoch 115: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0523 - accuracy: 0.0000e+00 - val_loss: 0.0247 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/500\n",
      "123/165 [=====================>........] - ETA: 0s - loss: 0.0487 - accuracy: 0.0000e+00\n",
      "Epoch 116: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/500\n",
      "124/165 [=====================>........] - ETA: 0s - loss: 0.0171 - accuracy: 0.0000e+00\n",
      "Epoch 117: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.1673 - accuracy: 0.0000e+00\n",
      "Epoch 118: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.5392 - accuracy: 0.0000e+00 - val_loss: 0.1600 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/500\n",
      "124/165 [=====================>........] - ETA: 0s - loss: 3.3418 - accuracy: 0.0000e+00\n",
      "Epoch 119: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 2.5298 - accuracy: 0.0000e+00 - val_loss: 0.0170 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/500\n",
      "122/165 [=====================>........] - ETA: 0s - loss: 0.0283 - accuracy: 0.0000e+00\n",
      "Epoch 120: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/500\n",
      "119/165 [====================>.........] - ETA: 0s - loss: 0.0308 - accuracy: 0.0000e+00\n",
      "Epoch 121: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0267 - accuracy: 0.0000e+00 - val_loss: 0.0701 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0164 - accuracy: 0.0000e+00\n",
      "Epoch 122: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0451 - accuracy: 0.0000e+00\n",
      "Epoch 123: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.0000e+00 - val_loss: 0.1943 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.0988 - accuracy: 0.0000e+00\n",
      "Epoch 124: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.0000e+00 - val_loss: 0.1280 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 1.7043 - accuracy: 0.0000e+00\n",
      "Epoch 125: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.5809 - accuracy: 0.0000e+00 - val_loss: 0.0174 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.0161 - accuracy: 0.0000e+00\n",
      "Epoch 126: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - val_loss: 0.0169 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 0.0000e+00\n",
      "Epoch 127: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.0000e+00 - val_loss: 0.0168 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.0152 - accuracy: 0.0000e+00\n",
      "Epoch 128: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.0000e+00 - val_loss: 0.0423 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0299 - accuracy: 0.0000e+00\n",
      "Epoch 129: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.0000e+00\n",
      "Epoch 130: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - val_loss: 0.0163 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.0000e+00\n",
      "Epoch 131: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0842 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0274 - accuracy: 0.0000e+00\n",
      "Epoch 132: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.0000e+00 - val_loss: 0.0347 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/500\n",
      "113/165 [===================>..........] - ETA: 0s - loss: 0.1764 - accuracy: 0.0000e+00\n",
      "Epoch 133: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.0000e+00 - val_loss: 0.2233 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.6104 - accuracy: 0.0000e+00\n",
      "Epoch 134: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.0000e+00 - val_loss: 0.0165 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/500\n",
      "121/165 [=====================>........] - ETA: 0s - loss: 0.0195 - accuracy: 0.0000e+00\n",
      "Epoch 135: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - val_loss: 0.0170 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.0284 - accuracy: 0.0000e+00\n",
      "Epoch 136: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.0000e+00 - val_loss: 0.0235 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0679 - accuracy: 0.0000e+00\n",
      "Epoch 137: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.0000e+00 - val_loss: 0.0164 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.9406 - accuracy: 0.0000e+00\n",
      "Epoch 138: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9084 - accuracy: 0.0000e+00 - val_loss: 0.0739 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.0190 - accuracy: 0.0000e+00\n",
      "Epoch 139: val_loss did not improve from 0.01521\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0681 - accuracy: 0.0000e+00\n",
      "Epoch 140: val_loss improved from 0.01521 to 0.01520, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.1957 - accuracy: 0.0000e+00\n",
      "Epoch 141: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1727 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0469 - accuracy: 0.0000e+00\n",
      "Epoch 142: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.0000e+00 - val_loss: 0.0166 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 143: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - val_loss: 0.0242 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0301 - accuracy: 0.0000e+00\n",
      "Epoch 144: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.9677 - accuracy: 0.0000e+00\n",
      "Epoch 145: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9557 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.0306 - accuracy: 0.0000e+00\n",
      "Epoch 146: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.0000e+00 - val_loss: 0.0169 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/500\n",
      "122/165 [=====================>........] - ETA: 0s - loss: 0.0468 - accuracy: 0.0000e+00\n",
      "Epoch 147: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.0000e+00 - val_loss: 0.0172 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.0162 - accuracy: 0.0000e+00\n",
      "Epoch 148: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0534 - accuracy: 0.0000e+00\n",
      "Epoch 149: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.0000e+00 - val_loss: 0.1546 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 5.0658 - accuracy: 0.0000e+00\n",
      "Epoch 150: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 4.3074 - accuracy: 0.0000e+00 - val_loss: 0.0462 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0427 - accuracy: 0.0000e+00\n",
      "Epoch 151: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.0000e+00\n",
      "Epoch 152: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - val_loss: 0.0172 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 153: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - val_loss: 0.0527 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.0208 - accuracy: 0.0000e+00\n",
      "Epoch 154: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0198 - accuracy: 0.0000e+00\n",
      "Epoch 155: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.0167 - accuracy: 0.0000e+00\n",
      "Epoch 156: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0213 - accuracy: 0.0000e+00\n",
      "Epoch 157: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.0000e+00\n",
      "Epoch 158: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0137 - accuracy: 0.0000e+00\n",
      "Epoch 159: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0136 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 160: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0174 - accuracy: 0.0000e+00\n",
      "Epoch 161: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.1289 - accuracy: 0.0000e+00\n",
      "Epoch 162: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0185 - accuracy: 0.0000e+00\n",
      "Epoch 163: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/500\n",
      "118/165 [====================>.........] - ETA: 0s - loss: 0.0145 - accuracy: 0.0000e+00\n",
      "Epoch 164: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 0.0000e+00 - val_loss: 0.0496 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.1808 - accuracy: 0.0000e+00\n",
      "Epoch 165: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1521 - accuracy: 0.0000e+00 - val_loss: 0.0779 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.0000e+00\n",
      "Epoch 166: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.0000e+00 - val_loss: 0.1277 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 2.3157 - accuracy: 0.0000e+00\n",
      "Epoch 167: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 2.0811 - accuracy: 0.0000e+00 - val_loss: 0.0293 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.0223 - accuracy: 0.0000e+00\n",
      "Epoch 168: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.0000e+00\n",
      "Epoch 169: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.0000e+00 - val_loss: 0.0706 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0225 - accuracy: 0.0000e+00\n",
      "Epoch 170: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.0000e+00 - val_loss: 0.0732 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 0.4830 - accuracy: 0.0000e+00\n",
      "Epoch 171: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.0000e+00 - val_loss: 0.0216 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.0000e+00\n",
      "Epoch 172: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.0000e+00 - val_loss: 0.0444 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.2001 - accuracy: 0.0000e+00\n",
      "Epoch 173: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0198 - accuracy: 0.0000e+00\n",
      "Epoch 174: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/500\n",
      "120/165 [====================>.........] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 175: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0136 - accuracy: 0.0000e+00\n",
      "Epoch 176: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.0000e+00 - val_loss: 0.2122 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/500\n",
      "127/165 [======================>.......] - ETA: 0s - loss: 2.4663 - accuracy: 0.0000e+00\n",
      "Epoch 177: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.9243 - accuracy: 0.0000e+00 - val_loss: 0.0217 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.3406 - accuracy: 0.0000e+00\n",
      "Epoch 178: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.0000e+00 - val_loss: 0.0165 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0187 - accuracy: 0.0000e+00\n",
      "Epoch 179: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0376 - accuracy: 0.0000e+00\n",
      "Epoch 180: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.0144 - accuracy: 0.0000e+00\n",
      "Epoch 181: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - val_loss: 0.0163 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.0000e+00\n",
      "Epoch 182: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0145 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.0204 - accuracy: 0.0000e+00\n",
      "Epoch 183: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - val_loss: 0.0246 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.1195 - accuracy: 0.0000e+00\n",
      "Epoch 184: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.0000e+00 - val_loss: 0.1198 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.1339 - accuracy: 0.0000e+00\n",
      "Epoch 185: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/500\n",
      "121/165 [=====================>........] - ETA: 0s - loss: 0.0137 - accuracy: 0.0000e+00\n",
      "Epoch 186: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.0000e+00 - val_loss: 0.0930 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0303 - accuracy: 0.0000e+00\n",
      "Epoch 187: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0000e+00 - val_loss: 0.0165 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0136 - accuracy: 0.0000e+00\n",
      "Epoch 188: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.8451 - accuracy: 0.0000e+00 - val_loss: 0.9595 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/500\n",
      "126/165 [=====================>........] - ETA: 0s - loss: 1.0772 - accuracy: 0.0000e+00\n",
      "Epoch 189: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9360 - accuracy: 0.0000e+00 - val_loss: 11.1092 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 18.7539 - accuracy: 0.0000e+00\n",
      "Epoch 190: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 16.5765 - accuracy: 0.0000e+00 - val_loss: 0.0171 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0162 - accuracy: 0.0000e+00\n",
      "Epoch 191: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.0000e+00 - val_loss: 0.0165 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/500\n",
      "115/165 [===================>..........] - ETA: 0s - loss: 0.0145 - accuracy: 0.0000e+00\n",
      "Epoch 192: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 0.0000e+00 - val_loss: 0.0264 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/500\n",
      "115/165 [===================>..........] - ETA: 0s - loss: 0.0169 - accuracy: 0.0000e+00\n",
      "Epoch 193: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/500\n",
      "114/165 [===================>..........] - ETA: 0s - loss: 0.0169 - accuracy: 0.0000e+00\n",
      "Epoch 194: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0142 - accuracy: 0.0000e+00\n",
      "Epoch 195: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0159 - accuracy: 0.0000e+00\n",
      "Epoch 196: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0274 - accuracy: 0.0000e+00\n",
      "Epoch 197: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0156 - accuracy: 0.0000e+00\n",
      "Epoch 198: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.0000e+00 - val_loss: 0.0579 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0535 - accuracy: 0.0000e+00\n",
      "Epoch 199: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.0000e+00 - val_loss: 0.0288 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.6312 - accuracy: 0.0000e+00\n",
      "Epoch 200: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/500\n",
      "122/165 [=====================>........] - ETA: 0s - loss: 0.0172 - accuracy: 0.0000e+00\n",
      "Epoch 201: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 202: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0135 - accuracy: 0.0000e+00\n",
      "Epoch 203: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0138 - accuracy: 0.0000e+00\n",
      "Epoch 204: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0134 - accuracy: 0.0000e+00\n",
      "Epoch 205: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.0136 - accuracy: 0.0000e+00\n",
      "Epoch 206: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0137 - accuracy: 0.0000e+00\n",
      "Epoch 207: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0208 - accuracy: 0.0000e+00\n",
      "Epoch 208: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0235 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.0179 - accuracy: 0.0000e+00\n",
      "Epoch 209: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/500\n",
      "126/165 [=====================>........] - ETA: 0s - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 210: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 211: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/500\n",
      "127/165 [======================>.......] - ETA: 0s - loss: 0.0156 - accuracy: 0.0000e+00\n",
      "Epoch 212: val_loss improved from 0.01520 to 0.01520, saving model to best_model.h5\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.0000e+00\n",
      "Epoch 213: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0380 - accuracy: 0.0000e+00\n",
      "Epoch 214: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.0226 - accuracy: 0.0000e+00\n",
      "Epoch 215: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0137 - accuracy: 0.0000e+00\n",
      "Epoch 216: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0329 - accuracy: 0.0000e+00 - val_loss: 1.0627 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.2762 - accuracy: 0.0000e+00\n",
      "Epoch 217: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.0000e+00 - val_loss: 0.0324 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 1.0795 - accuracy: 0.0000e+00\n",
      "Epoch 218: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9558 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0168 - accuracy: 0.0000e+00\n",
      "Epoch 219: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0192 - accuracy: 0.0000e+00\n",
      "Epoch 220: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0141 - accuracy: 0.0000e+00\n",
      "Epoch 221: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - val_loss: 0.0226 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0167 - accuracy: 0.0000e+00\n",
      "Epoch 222: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - val_loss: 0.0687 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 2.1671 - accuracy: 0.0000e+00\n",
      "Epoch 223: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 2.0531 - accuracy: 0.0000e+00 - val_loss: 0.0730 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.0221 - accuracy: 0.0000e+00\n",
      "Epoch 224: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 0.0169 - accuracy: 0.0000e+00\n",
      "Epoch 225: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0165 - accuracy: 0.0000e+00\n",
      "Epoch 226: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0143 - accuracy: 0.0000e+00\n",
      "Epoch 227: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/500\n",
      "122/165 [=====================>........] - ETA: 0s - loss: 0.0146 - accuracy: 0.0000e+00\n",
      "Epoch 228: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.0000e+00 - val_loss: 0.0188 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0186 - accuracy: 0.0000e+00\n",
      "Epoch 229: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - val_loss: 0.0214 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.1037 - accuracy: 0.0000e+00\n",
      "Epoch 230: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.0000e+00 - val_loss: 0.0825 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 2.5177 - accuracy: 0.0000e+00\n",
      "Epoch 231: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 2.2734 - accuracy: 0.0000e+00 - val_loss: 0.0173 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0431 - accuracy: 0.0000e+00\n",
      "Epoch 232: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0295 - accuracy: 0.0000e+00\n",
      "Epoch 233: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.0000e+00 - val_loss: 0.1527 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.2268 - accuracy: 0.0000e+00\n",
      "Epoch 234: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2208 - accuracy: 0.0000e+00 - val_loss: 0.0370 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 235: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0164 - accuracy: 0.0000e+00\n",
      "Epoch 236: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0165 - accuracy: 0.0000e+00\n",
      "Epoch 237: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0842 - accuracy: 0.0000e+00\n",
      "Epoch 238: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.0000e+00 - val_loss: 0.0174 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0283 - accuracy: 0.0000e+00\n",
      "Epoch 239: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0841 - accuracy: 0.0000e+00\n",
      "Epoch 240: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/500\n",
      "123/165 [=====================>........] - ETA: 0s - loss: 0.0937 - accuracy: 0.0000e+00\n",
      "Epoch 241: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.0000e+00 - val_loss: 0.0279 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 1.3757 - accuracy: 0.0000e+00\n",
      "Epoch 242: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.4081 - accuracy: 0.0000e+00 - val_loss: 1.8133 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 0.4735 - accuracy: 0.0000e+00\n",
      "Epoch 243: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.0000e+00 - val_loss: 0.0204 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/500\n",
      "108/165 [==================>...........] - ETA: 0s - loss: 0.3059 - accuracy: 0.0000e+00\n",
      "Epoch 244: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2597 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0950 - accuracy: 0.0000e+00\n",
      "Epoch 245: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0808 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/500\n",
      "127/165 [======================>.......] - ETA: 0s - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 246: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0178 - accuracy: 0.0000e+00\n",
      "Epoch 247: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0222 - accuracy: 0.0000e+00\n",
      "Epoch 248: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.3008 - accuracy: 0.0000e+00\n",
      "Epoch 249: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.2892 - accuracy: 0.0000e+00 - val_loss: 0.0174 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0180 - accuracy: 0.0000e+00\n",
      "Epoch 250: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.0000e+00\n",
      "Epoch 251: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0320 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0243 - accuracy: 0.0000e+00\n",
      "Epoch 252: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.4008 - accuracy: 0.0000e+00 - val_loss: 17.5297 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 3.0592 - accuracy: 0.0000e+00\n",
      "Epoch 253: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 2.8743 - accuracy: 0.0000e+00 - val_loss: 0.0317 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 1.5350 - accuracy: 0.0000e+00\n",
      "Epoch 254: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 1.3969 - accuracy: 0.0000e+00 - val_loss: 0.0167 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/500\n",
      "124/165 [=====================>........] - ETA: 0s - loss: 0.0180 - accuracy: 0.0000e+00\n",
      "Epoch 255: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0140 - accuracy: 0.0000e+00\n",
      "Epoch 256: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 257/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0138 - accuracy: 0.0000e+00\n",
      "Epoch 257: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 258/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 0.0287 - accuracy: 0.0000e+00\n",
      "Epoch 258: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0000e+00 - val_loss: 0.0177 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.2198 - accuracy: 0.0000e+00\n",
      "Epoch 259: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 260/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0151 - accuracy: 0.0000e+00\n",
      "Epoch 260: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 261/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 261: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 262/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0204 - accuracy: 0.0000e+00\n",
      "Epoch 262: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - val_loss: 0.0293 - val_accuracy: 0.0000e+00\n",
      "Epoch 263/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0152 - accuracy: 0.0000e+00\n",
      "Epoch 263: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 264/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0141 - accuracy: 0.0000e+00\n",
      "Epoch 264: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.0000e+00 - val_loss: 0.1488 - val_accuracy: 0.0000e+00\n",
      "Epoch 265/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.1357 - accuracy: 0.0000e+00\n",
      "Epoch 265: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 266/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0297 - accuracy: 0.0000e+00\n",
      "Epoch 266: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 267/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.0142 - accuracy: 0.0000e+00\n",
      "Epoch 267: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 268/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.0000e+00\n",
      "Epoch 268: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - val_loss: 0.0181 - val_accuracy: 0.0000e+00\n",
      "Epoch 269/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0325 - accuracy: 0.0000e+00\n",
      "Epoch 269: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.0000e+00 - val_loss: 0.0163 - val_accuracy: 0.0000e+00\n",
      "Epoch 270/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.0319 - accuracy: 0.0000e+00\n",
      "Epoch 270: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 271/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.6460 - accuracy: 0.0000e+00\n",
      "Epoch 271: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1.4234 - accuracy: 0.0000e+00 - val_loss: 1.8674 - val_accuracy: 0.0000e+00\n",
      "Epoch 272/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.2453 - accuracy: 0.0000e+00\n",
      "Epoch 272: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.0000e+00 - val_loss: 0.0335 - val_accuracy: 0.0000e+00\n",
      "Epoch 273/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0332 - accuracy: 0.0000e+00\n",
      "Epoch 273: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.0000e+00 - val_loss: 1.5479 - val_accuracy: 0.0000e+00\n",
      "Epoch 274/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.5919 - accuracy: 0.0000e+00\n",
      "Epoch 274: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.0000e+00 - val_loss: 0.0254 - val_accuracy: 0.0000e+00\n",
      "Epoch 275/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0587 - accuracy: 0.0000e+00\n",
      "Epoch 275: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.0000e+00 - val_loss: 0.4194 - val_accuracy: 0.0000e+00\n",
      "Epoch 276/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.2606 - accuracy: 0.0000e+00\n",
      "Epoch 276: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 277/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0213 - accuracy: 0.0000e+00\n",
      "Epoch 277: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 278/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 278: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 279/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0142 - accuracy: 0.0000e+00\n",
      "Epoch 279: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 280/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0134 - accuracy: 0.0000e+00\n",
      "Epoch 280: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.0000e+00 - val_loss: 0.0168 - val_accuracy: 0.0000e+00\n",
      "Epoch 281/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.0336 - accuracy: 0.0000e+00\n",
      "Epoch 281: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0336 - accuracy: 0.0000e+00 - val_loss: 0.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 282/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 282: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 283/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 2.3437 - accuracy: 0.0000e+00\n",
      "Epoch 283: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 2.1880 - accuracy: 0.0000e+00 - val_loss: 0.0235 - val_accuracy: 0.0000e+00\n",
      "Epoch 284/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0651 - accuracy: 0.0000e+00\n",
      "Epoch 284: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0587 - accuracy: 0.0000e+00 - val_loss: 0.0187 - val_accuracy: 0.0000e+00\n",
      "Epoch 285/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0373 - accuracy: 0.0000e+00\n",
      "Epoch 285: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.0000e+00 - val_loss: 0.2196 - val_accuracy: 0.0000e+00\n",
      "Epoch 286/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.5979 - accuracy: 0.0000e+00\n",
      "Epoch 286: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.0000e+00 - val_loss: 0.0179 - val_accuracy: 0.0000e+00\n",
      "Epoch 287/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.0155 - accuracy: 0.0000e+00\n",
      "Epoch 287: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 288/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0147 - accuracy: 0.0000e+00\n",
      "Epoch 288: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 289/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.0000e+00\n",
      "Epoch 289: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - val_loss: 0.0201 - val_accuracy: 0.0000e+00\n",
      "Epoch 290/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 0.2807 - accuracy: 0.0000e+00\n",
      "Epoch 290: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
      "Epoch 291/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0171 - accuracy: 0.0000e+00\n",
      "Epoch 291: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 292/500\n",
      "128/165 [======================>.......] - ETA: 0s - loss: 0.0176 - accuracy: 0.0000e+00\n",
      "Epoch 292: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.0000e+00 - val_loss: 2.8460 - val_accuracy: 0.0000e+00\n",
      "Epoch 293/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 3.2403 - accuracy: 0.0000e+00\n",
      "Epoch 293: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 3.1606 - accuracy: 0.0000e+00 - val_loss: 0.0199 - val_accuracy: 0.0000e+00\n",
      "Epoch 294/500\n",
      "120/165 [====================>.........] - ETA: 0s - loss: 0.0434 - accuracy: 0.0000e+00\n",
      "Epoch 294: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0352 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 295/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.0552 - accuracy: 0.0000e+00\n",
      "Epoch 295: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 296/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0200 - accuracy: 0.0000e+00\n",
      "Epoch 296: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\n",
      "Epoch 297/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0155 - accuracy: 0.0000e+00\n",
      "Epoch 297: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 298/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0197 - accuracy: 0.0000e+00\n",
      "Epoch 298: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - val_loss: 0.0211 - val_accuracy: 0.0000e+00\n",
      "Epoch 299/500\n",
      "124/165 [=====================>........] - ETA: 0s - loss: 0.0216 - accuracy: 0.0000e+00\n",
      "Epoch 299: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0339 - accuracy: 0.0000e+00 - val_loss: 0.0573 - val_accuracy: 0.0000e+00\n",
      "Epoch 300/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0427 - accuracy: 0.0000e+00\n",
      "Epoch 300: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1238 - accuracy: 0.0000e+00 - val_loss: 0.9490 - val_accuracy: 0.0000e+00\n",
      "Epoch 301/500\n",
      "126/165 [=====================>........] - ETA: 0s - loss: 2.0637 - accuracy: 0.0000e+00\n",
      "Epoch 301: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.6438 - accuracy: 0.0000e+00 - val_loss: 0.0962 - val_accuracy: 0.0000e+00\n",
      "Epoch 302/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0490 - accuracy: 0.0000e+00\n",
      "Epoch 302: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0483 - accuracy: 0.0000e+00 - val_loss: 0.0235 - val_accuracy: 0.0000e+00\n",
      "Epoch 303/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.6120 - accuracy: 0.0000e+00\n",
      "Epoch 303: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 304/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.0211 - accuracy: 0.0000e+00\n",
      "Epoch 304: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 305/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0239 - accuracy: 0.0000e+00\n",
      "Epoch 305: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 306/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.0309 - accuracy: 0.0000e+00\n",
      "Epoch 306: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 307/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 307: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
      "Epoch 308/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0983 - accuracy: 0.0000e+00\n",
      "Epoch 308: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0931 - accuracy: 0.0000e+00 - val_loss: 0.0215 - val_accuracy: 0.0000e+00\n",
      "Epoch 309/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0433 - accuracy: 0.0000e+00\n",
      "Epoch 309: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 310/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.0139 - accuracy: 0.0000e+00\n",
      "Epoch 310: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - val_loss: 0.1024 - val_accuracy: 0.0000e+00\n",
      "Epoch 311/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0224 - accuracy: 0.0000e+00\n",
      "Epoch 311: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - val_loss: 0.1603 - val_accuracy: 0.0000e+00\n",
      "Epoch 312/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0347 - accuracy: 0.0000e+00\n",
      "Epoch 312: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.0000e+00 - val_loss: 0.0187 - val_accuracy: 0.0000e+00\n",
      "Epoch 313/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0214 - accuracy: 0.0000e+00\n",
      "Epoch 313: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 314/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0974 - accuracy: 0.0000e+00\n",
      "Epoch 314: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0828 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 315/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.0000e+00\n",
      "Epoch 315: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - val_loss: 0.0186 - val_accuracy: 0.0000e+00\n",
      "Epoch 316/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.1870 - accuracy: 0.0000e+00\n",
      "Epoch 316: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.0000e+00 - val_loss: 0.7925 - val_accuracy: 0.0000e+00\n",
      "Epoch 317/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 6.7522 - accuracy: 0.0000e+00 \n",
      "Epoch 317: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 5.3968 - accuracy: 0.0000e+00 - val_loss: 0.0189 - val_accuracy: 0.0000e+00\n",
      "Epoch 318/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0161 - accuracy: 0.0000e+00\n",
      "Epoch 318: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.0000e+00 - val_loss: 0.0170 - val_accuracy: 0.0000e+00\n",
      "Epoch 319/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0339 - accuracy: 0.0000e+00\n",
      "Epoch 319: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 320/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.2916 - accuracy: 0.0000e+00\n",
      "Epoch 320: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.2846 - accuracy: 0.0000e+00 - val_loss: 0.0170 - val_accuracy: 0.0000e+00\n",
      "Epoch 321/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0285 - accuracy: 0.0000e+00\n",
      "Epoch 321: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 322/500\n",
      "127/165 [======================>.......] - ETA: 0s - loss: 0.0138 - accuracy: 0.0000e+00\n",
      "Epoch 322: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.0000e+00 - val_loss: 0.0218 - val_accuracy: 0.0000e+00\n",
      "Epoch 323/500\n",
      "123/165 [=====================>........] - ETA: 0s - loss: 0.0213 - accuracy: 0.0000e+00\n",
      "Epoch 323: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - val_loss: 0.0166 - val_accuracy: 0.0000e+00\n",
      "Epoch 324/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 324: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 325/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0201 - accuracy: 0.0000e+00\n",
      "Epoch 325: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 326/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 0.0000e+00\n",
      "Epoch 326: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.0000e+00 - val_loss: 0.1089 - val_accuracy: 0.0000e+00\n",
      "Epoch 327/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.1562 - accuracy: 0.0000e+00\n",
      "Epoch 327: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 328/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.0176 - accuracy: 0.0000e+00\n",
      "Epoch 328: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - val_loss: 0.0169 - val_accuracy: 0.0000e+00\n",
      "Epoch 329/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0540 - accuracy: 0.0000e+00\n",
      "Epoch 329: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.0000e+00 - val_loss: 0.0270 - val_accuracy: 0.0000e+00\n",
      "Epoch 330/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.0235 - accuracy: 0.0000e+00\n",
      "Epoch 330: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.0000e+00 - val_loss: 0.0325 - val_accuracy: 0.0000e+00\n",
      "Epoch 331/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.4141 - accuracy: 0.0000e+00\n",
      "Epoch 331: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.0000e+00 - val_loss: 0.0192 - val_accuracy: 0.0000e+00\n",
      "Epoch 332/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.0219 - accuracy: 0.0000e+00\n",
      "Epoch 332: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 333/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0172 - accuracy: 0.0000e+00\n",
      "Epoch 333: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 334/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.0534 - accuracy: 0.0000e+00\n",
      "Epoch 334: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 335/500\n",
      "126/165 [=====================>........] - ETA: 0s - loss: 0.0280 - accuracy: 0.0000e+00\n",
      "Epoch 335: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.0000e+00 - val_loss: 0.0270 - val_accuracy: 0.0000e+00\n",
      "Epoch 336/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0262 - accuracy: 0.0000e+00\n",
      "Epoch 336: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2379 - accuracy: 0.0000e+00 - val_loss: 0.0574 - val_accuracy: 0.0000e+00\n",
      "Epoch 337/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.2777 - accuracy: 0.0000e+00\n",
      "Epoch 337: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2626 - accuracy: 0.0000e+00 - val_loss: 0.1463 - val_accuracy: 0.0000e+00\n",
      "Epoch 338/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 1.2929 - accuracy: 0.0000e+00\n",
      "Epoch 338: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.0522 - accuracy: 0.0000e+00 - val_loss: 0.0194 - val_accuracy: 0.0000e+00\n",
      "Epoch 339/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0413 - accuracy: 0.0000e+00\n",
      "Epoch 339: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.0000e+00 - val_loss: 0.0205 - val_accuracy: 0.0000e+00\n",
      "Epoch 340/500\n",
      "140/165 [========================>.....] - ETA: 0s - loss: 0.4712 - accuracy: 0.0000e+00\n",
      "Epoch 340: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.0000e+00 - val_loss: 0.0169 - val_accuracy: 0.0000e+00\n",
      "Epoch 341/500\n",
      "127/165 [======================>.......] - ETA: 0s - loss: 0.0319 - accuracy: 0.0000e+00\n",
      "Epoch 341: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.0000e+00 - val_loss: 0.0178 - val_accuracy: 0.0000e+00\n",
      "Epoch 342/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.0201 - accuracy: 0.0000e+00\n",
      "Epoch 342: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - val_loss: 0.0611 - val_accuracy: 0.0000e+00\n",
      "Epoch 343/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.0258 - accuracy: 0.0000e+00\n",
      "Epoch 343: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 344/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0180 - accuracy: 0.0000e+00\n",
      "Epoch 344: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0290 - accuracy: 0.0000e+00 - val_loss: 0.0240 - val_accuracy: 0.0000e+00\n",
      "Epoch 345/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0272 - accuracy: 0.0000e+00\n",
      "Epoch 345: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.0000e+00 - val_loss: 0.0270 - val_accuracy: 0.0000e+00\n",
      "Epoch 346/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.3007 - accuracy: 0.0000e+00\n",
      "Epoch 346: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.0000e+00 - val_loss: 2.8481 - val_accuracy: 0.0000e+00\n",
      "Epoch 347/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 6.9450 - accuracy: 0.0000e+00\n",
      "Epoch 347: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 6.6047 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 348/500\n",
      "120/165 [====================>.........] - ETA: 0s - loss: 0.0189 - accuracy: 0.0000e+00\n",
      "Epoch 348: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 349/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 349: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 350/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.0000e+00\n",
      "Epoch 350: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 351/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0144 - accuracy: 0.0000e+00\n",
      "Epoch 351: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 352/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 352: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 353/500\n",
      "129/165 [======================>.......] - ETA: 0s - loss: 0.0138 - accuracy: 0.0000e+00\n",
      "Epoch 353: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 354/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0134 - accuracy: 0.0000e+00\n",
      "Epoch 354: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.0000e+00 - val_loss: 0.0183 - val_accuracy: 0.0000e+00\n",
      "Epoch 355/500\n",
      "125/165 [=====================>........] - ETA: 0s - loss: 0.0140 - accuracy: 0.0000e+00\n",
      "Epoch 355: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.0000e+00 - val_loss: 0.0334 - val_accuracy: 0.0000e+00\n",
      "Epoch 356/500\n",
      "123/165 [=====================>........] - ETA: 0s - loss: 0.0154 - accuracy: 0.0000e+00\n",
      "Epoch 356: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - val_loss: 0.0186 - val_accuracy: 0.0000e+00\n",
      "Epoch 357/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.0166 - accuracy: 0.0000e+00\n",
      "Epoch 357: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.0000e+00 - val_loss: 0.0225 - val_accuracy: 0.0000e+00\n",
      "Epoch 358/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0160 - accuracy: 0.0000e+00\n",
      "Epoch 358: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - val_loss: 0.0249 - val_accuracy: 0.0000e+00\n",
      "Epoch 359/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 1.9218 - accuracy: 0.0000e+00\n",
      "Epoch 359: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.5420 - accuracy: 0.0000e+00 - val_loss: 0.0190 - val_accuracy: 0.0000e+00\n",
      "Epoch 360/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0186 - accuracy: 0.0000e+00\n",
      "Epoch 360: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - val_loss: 0.0251 - val_accuracy: 0.0000e+00\n",
      "Epoch 361/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 361: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.0000e+00 - val_loss: 0.0424 - val_accuracy: 0.0000e+00\n",
      "Epoch 362/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0191 - accuracy: 0.0000e+00\n",
      "Epoch 362: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 363/500\n",
      "121/165 [=====================>........] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 363: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 364/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0214 - accuracy: 0.0000e+00\n",
      "Epoch 364: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - val_loss: 0.0270 - val_accuracy: 0.0000e+00\n",
      "Epoch 365/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0824 - accuracy: 0.0000e+00\n",
      "Epoch 365: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.0000e+00 - val_loss: 0.0362 - val_accuracy: 0.0000e+00\n",
      "Epoch 366/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.2841 - accuracy: 0.0000e+00\n",
      "Epoch 366: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.0000e+00 - val_loss: 0.0198 - val_accuracy: 0.0000e+00\n",
      "Epoch 367/500\n",
      "124/165 [=====================>........] - ETA: 0s - loss: 0.1904 - accuracy: 0.0000e+00\n",
      "Epoch 367: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1474 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 368/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0139 - accuracy: 0.0000e+00\n",
      "Epoch 368: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.0000e+00 - val_loss: 0.0465 - val_accuracy: 0.0000e+00\n",
      "Epoch 369/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.7521 - accuracy: 0.0000e+00\n",
      "Epoch 369: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.7240 - accuracy: 0.0000e+00 - val_loss: 0.0282 - val_accuracy: 0.0000e+00\n",
      "Epoch 370/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 0.1605 - accuracy: 0.0000e+00\n",
      "Epoch 370: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1992 - accuracy: 0.0000e+00 - val_loss: 0.0854 - val_accuracy: 0.0000e+00\n",
      "Epoch 371/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.5338 - accuracy: 0.0000e+00\n",
      "Epoch 371: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.0000e+00 - val_loss: 0.0368 - val_accuracy: 0.0000e+00\n",
      "Epoch 372/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.0000e+00\n",
      "Epoch 372: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1267 - accuracy: 0.0000e+00 - val_loss: 0.0215 - val_accuracy: 0.0000e+00\n",
      "Epoch 373/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0224 - accuracy: 0.0000e+00\n",
      "Epoch 373: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 374/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0399 - accuracy: 0.0000e+00\n",
      "Epoch 374: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 0.0000e+00 - val_loss: 0.0243 - val_accuracy: 0.0000e+00\n",
      "Epoch 375/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0156 - accuracy: 0.0000e+00\n",
      "Epoch 375: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - val_loss: 0.0895 - val_accuracy: 0.0000e+00\n",
      "Epoch 376/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.1856 - accuracy: 0.0000e+00\n",
      "Epoch 376: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1810 - accuracy: 0.0000e+00 - val_loss: 0.0256 - val_accuracy: 0.0000e+00\n",
      "Epoch 377/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.0542 - accuracy: 0.0000e+00\n",
      "Epoch 377: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 378/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0660 - accuracy: 0.0000e+00\n",
      "Epoch 378: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 379/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.1511 - accuracy: 0.0000e+00\n",
      "Epoch 379: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1470 - accuracy: 0.0000e+00 - val_loss: 0.0207 - val_accuracy: 0.0000e+00\n",
      "Epoch 380/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0701 - accuracy: 0.0000e+00\n",
      "Epoch 380: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.0000e+00 - val_loss: 0.0230 - val_accuracy: 0.0000e+00\n",
      "Epoch 381/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.0161 - accuracy: 0.0000e+00\n",
      "Epoch 381: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.0000e+00 - val_loss: 0.1509 - val_accuracy: 0.0000e+00\n",
      "Epoch 382/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.1905 - accuracy: 0.0000e+00\n",
      "Epoch 382: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.0000e+00 - val_loss: 0.0420 - val_accuracy: 0.0000e+00\n",
      "Epoch 383/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 1.1655 - accuracy: 0.0000e+00\n",
      "Epoch 383: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9828 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 384/500\n",
      "119/165 [====================>.........] - ETA: 0s - loss: 0.0160 - accuracy: 0.0000e+00\n",
      "Epoch 384: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 385/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0978 - accuracy: 0.0000e+00\n",
      "Epoch 385: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.0000e+00 - val_loss: 0.0209 - val_accuracy: 0.0000e+00\n",
      "Epoch 386/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.8161 - accuracy: 0.0000e+00\n",
      "Epoch 386: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.7233 - accuracy: 0.0000e+00 - val_loss: 0.0163 - val_accuracy: 0.0000e+00\n",
      "Epoch 387/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0306 - accuracy: 0.0000e+00\n",
      "Epoch 387: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.0000e+00 - val_loss: 0.0224 - val_accuracy: 0.0000e+00\n",
      "Epoch 388/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0157 - accuracy: 0.0000e+00\n",
      "Epoch 388: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.0000e+00 - val_loss: 0.0190 - val_accuracy: 0.0000e+00\n",
      "Epoch 389/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 0.0299 - accuracy: 0.0000e+00\n",
      "Epoch 389: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.0000e+00 - val_loss: 0.0178 - val_accuracy: 0.0000e+00\n",
      "Epoch 390/500\n",
      "119/165 [====================>.........] - ETA: 0s - loss: 0.9158 - accuracy: 0.0000e+00\n",
      "Epoch 390: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9626 - accuracy: 0.0000e+00 - val_loss: 0.0500 - val_accuracy: 0.0000e+00\n",
      "Epoch 391/500\n",
      "162/165 [============================>.] - ETA: 0s - loss: 2.1399 - accuracy: 0.0000e+00\n",
      "Epoch 391: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 2.1133 - accuracy: 0.0000e+00 - val_loss: 0.0172 - val_accuracy: 0.0000e+00\n",
      "Epoch 392/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.0200 - accuracy: 0.0000e+00\n",
      "Epoch 392: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\n",
      "Epoch 393/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0157 - accuracy: 0.0000e+00\n",
      "Epoch 393: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.0000e+00 - val_loss: 0.0224 - val_accuracy: 0.0000e+00\n",
      "Epoch 394/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0144 - accuracy: 0.0000e+00\n",
      "Epoch 394: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 395/500\n",
      "120/165 [====================>.........] - ETA: 0s - loss: 0.0237 - accuracy: 0.0000e+00\n",
      "Epoch 395: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 396/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 396: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 397/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 0.0140 - accuracy: 0.0000e+00\n",
      "Epoch 397: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - val_loss: 0.0588 - val_accuracy: 0.0000e+00\n",
      "Epoch 398/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0595 - accuracy: 0.0000e+00\n",
      "Epoch 398: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\n",
      "Epoch 399/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.0000e+00\n",
      "Epoch 399: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 400/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0157 - accuracy: 0.0000e+00\n",
      "Epoch 400: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 401/500\n",
      "133/165 [=======================>......] - ETA: 0s - loss: 0.0233 - accuracy: 0.0000e+00\n",
      "Epoch 401: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - val_loss: 0.0533 - val_accuracy: 0.0000e+00\n",
      "Epoch 402/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0839 - accuracy: 0.0000e+00\n",
      "Epoch 402: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.0000e+00 - val_loss: 0.0168 - val_accuracy: 0.0000e+00\n",
      "Epoch 403/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 1.0998 - accuracy: 0.0000e+00\n",
      "Epoch 403: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.9934 - accuracy: 0.0000e+00 - val_loss: 0.0168 - val_accuracy: 0.0000e+00\n",
      "Epoch 404/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.0334 - accuracy: 0.0000e+00\n",
      "Epoch 404: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0321 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 405/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0280 - accuracy: 0.0000e+00\n",
      "Epoch 405: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 406/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.0000e+00\n",
      "Epoch 406: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.0000e+00 - val_loss: 0.0193 - val_accuracy: 0.0000e+00\n",
      "Epoch 407/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.7793 - accuracy: 0.0000e+00\n",
      "Epoch 407: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.8140 - accuracy: 0.0000e+00 - val_loss: 0.3008 - val_accuracy: 0.0000e+00\n",
      "Epoch 408/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0475 - accuracy: 0.0000e+00\n",
      "Epoch 408: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0433 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 409/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.0000e+00\n",
      "Epoch 409: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 0.0000e+00 - val_loss: 0.0303 - val_accuracy: 0.0000e+00\n",
      "Epoch 410/500\n",
      "144/165 [=========================>....] - ETA: 0s - loss: 0.8740 - accuracy: 0.0000e+00\n",
      "Epoch 410: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.7709 - accuracy: 0.0000e+00 - val_loss: 0.0249 - val_accuracy: 0.0000e+00\n",
      "Epoch 411/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.4695 - accuracy: 0.0000e+00\n",
      "Epoch 411: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.4693 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 412/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 0.0663 - accuracy: 0.0000e+00\n",
      "Epoch 412: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.0000e+00 - val_loss: 0.1341 - val_accuracy: 0.0000e+00\n",
      "Epoch 413/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.3195 - accuracy: 0.0000e+00\n",
      "Epoch 413: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.0000e+00 - val_loss: 0.0844 - val_accuracy: 0.0000e+00\n",
      "Epoch 414/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0246 - accuracy: 0.0000e+00\n",
      "Epoch 414: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.0000e+00 - val_loss: 0.0180 - val_accuracy: 0.0000e+00\n",
      "Epoch 415/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.0000e+00\n",
      "Epoch 415: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - val_loss: 0.0161 - val_accuracy: 0.0000e+00\n",
      "Epoch 416/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0146 - accuracy: 0.0000e+00\n",
      "Epoch 416: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - val_loss: 0.0367 - val_accuracy: 0.0000e+00\n",
      "Epoch 417/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0798 - accuracy: 0.0000e+00\n",
      "Epoch 417: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.0000e+00 - val_loss: 0.0787 - val_accuracy: 0.0000e+00\n",
      "Epoch 418/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 1.1532 - accuracy: 0.0000e+00\n",
      "Epoch 418: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.0708 - accuracy: 0.0000e+00 - val_loss: 0.1100 - val_accuracy: 0.0000e+00\n",
      "Epoch 419/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.0194 - accuracy: 0.0000e+00\n",
      "Epoch 419: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - val_loss: 0.0223 - val_accuracy: 0.0000e+00\n",
      "Epoch 420/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.1037 - accuracy: 0.0000e+00\n",
      "Epoch 420: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1053 - accuracy: 0.0000e+00 - val_loss: 0.0965 - val_accuracy: 0.0000e+00\n",
      "Epoch 421/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.0000e+00\n",
      "Epoch 421: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.0000e+00 - val_loss: 0.0208 - val_accuracy: 0.0000e+00\n",
      "Epoch 422/500\n",
      "164/165 [============================>.] - ETA: 0s - loss: 0.5060 - accuracy: 0.0000e+00\n",
      "Epoch 422: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.0000e+00 - val_loss: 0.0306 - val_accuracy: 0.0000e+00\n",
      "Epoch 423/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.0630 - accuracy: 0.0000e+00\n",
      "Epoch 423: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 424/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 424: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - val_loss: 0.0167 - val_accuracy: 0.0000e+00\n",
      "Epoch 425/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.3696 - accuracy: 0.0000e+00\n",
      "Epoch 425: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3157 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 426/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0162 - accuracy: 0.0000e+00\n",
      "Epoch 426: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.0000e+00 - val_loss: 0.0193 - val_accuracy: 0.0000e+00\n",
      "Epoch 427/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 0.0839 - accuracy: 0.0000e+00\n",
      "Epoch 427: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0703 - accuracy: 0.0000e+00 - val_loss: 0.0180 - val_accuracy: 0.0000e+00\n",
      "Epoch 428/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0736 - accuracy: 0.0000e+00\n",
      "Epoch 428: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.0000e+00 - val_loss: 0.0704 - val_accuracy: 0.0000e+00\n",
      "Epoch 429/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 2.2761 - accuracy: 0.0000e+00\n",
      "Epoch 429: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 2.1384 - accuracy: 0.0000e+00 - val_loss: 0.0208 - val_accuracy: 0.0000e+00\n",
      "Epoch 430/500\n",
      "148/165 [=========================>....] - ETA: 0s - loss: 0.0226 - accuracy: 0.0000e+00\n",
      "Epoch 430: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - val_loss: 0.0152 - val_accuracy: 0.0000e+00\n",
      "Epoch 431/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.0148 - accuracy: 0.0000e+00\n",
      "Epoch 431: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 432/500\n",
      "138/165 [========================>.....] - ETA: 0s - loss: 0.0180 - accuracy: 0.0000e+00\n",
      "Epoch 432: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - val_loss: 0.0372 - val_accuracy: 0.0000e+00\n",
      "Epoch 433/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.0000e+00\n",
      "Epoch 433: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 434/500\n",
      "150/165 [==========================>...] - ETA: 0s - loss: 0.0241 - accuracy: 0.0000e+00\n",
      "Epoch 434: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - val_loss: 0.0330 - val_accuracy: 0.0000e+00\n",
      "Epoch 435/500\n",
      "134/165 [=======================>......] - ETA: 0s - loss: 0.0251 - accuracy: 0.0000e+00\n",
      "Epoch 435: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - val_loss: 0.0474 - val_accuracy: 0.0000e+00\n",
      "Epoch 436/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.4650 - accuracy: 0.0000e+00\n",
      "Epoch 436: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.0000e+00 - val_loss: 0.0427 - val_accuracy: 0.0000e+00\n",
      "Epoch 437/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.1249 - accuracy: 0.0000e+00\n",
      "Epoch 437: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.0000e+00 - val_loss: 0.0607 - val_accuracy: 0.0000e+00\n",
      "Epoch 438/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.3999 - accuracy: 0.0000e+00\n",
      "Epoch 438: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.0000e+00 - val_loss: 0.0449 - val_accuracy: 0.0000e+00\n",
      "Epoch 439/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0259 - accuracy: 0.0000e+00\n",
      "Epoch 439: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0467 - accuracy: 0.0000e+00 - val_loss: 0.1116 - val_accuracy: 0.0000e+00\n",
      "Epoch 440/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.8640 - accuracy: 0.0000e+00\n",
      "Epoch 440: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.8288 - accuracy: 0.0000e+00 - val_loss: 0.0511 - val_accuracy: 0.0000e+00\n",
      "Epoch 441/500\n",
      "126/165 [=====================>........] - ETA: 0s - loss: 0.1987 - accuracy: 0.0000e+00\n",
      "Epoch 441: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1581 - accuracy: 0.0000e+00 - val_loss: 0.0550 - val_accuracy: 0.0000e+00\n",
      "Epoch 442/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.0000e+00\n",
      "Epoch 442: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 443/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0151 - accuracy: 0.0000e+00\n",
      "Epoch 443: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.0000e+00 - val_loss: 0.0271 - val_accuracy: 0.0000e+00\n",
      "Epoch 444/500\n",
      "161/165 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.0000e+00\n",
      "Epoch 444: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 445/500\n",
      "151/165 [==========================>...] - ETA: 0s - loss: 0.0153 - accuracy: 0.0000e+00\n",
      "Epoch 445: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 446/500\n",
      "136/165 [=======================>......] - ETA: 0s - loss: 0.0337 - accuracy: 0.0000e+00\n",
      "Epoch 446: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.0000e+00 - val_loss: 0.0165 - val_accuracy: 0.0000e+00\n",
      "Epoch 447/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0136 - accuracy: 0.0000e+00\n",
      "Epoch 447: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 448/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.0733 - accuracy: 0.0000e+00\n",
      "Epoch 448: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 449/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.0143 - accuracy: 0.0000e+00\n",
      "Epoch 449: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.0000e+00 - val_loss: 0.0977 - val_accuracy: 0.0000e+00\n",
      "Epoch 450/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 6.0987 - accuracy: 0.0000e+00\n",
      "Epoch 450: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 5.2486 - accuracy: 0.0000e+00 - val_loss: 0.0189 - val_accuracy: 0.0000e+00\n",
      "Epoch 451/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.0163 - accuracy: 0.0000e+00\n",
      "Epoch 451: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 452/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.0625 - accuracy: 0.0000e+00\n",
      "Epoch 452: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 453/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0172 - accuracy: 0.0000e+00\n",
      "Epoch 453: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.0000e+00 - val_loss: 0.0530 - val_accuracy: 0.0000e+00\n",
      "Epoch 454/500\n",
      "139/165 [========================>.....] - ETA: 0s - loss: 0.1313 - accuracy: 0.0000e+00\n",
      "Epoch 454: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.0000e+00 - val_loss: 0.0203 - val_accuracy: 0.0000e+00\n",
      "Epoch 455/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.0000e+00\n",
      "Epoch 455: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 456/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0542 - accuracy: 0.0000e+00\n",
      "Epoch 456: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.0000e+00 - val_loss: 0.0345 - val_accuracy: 0.0000e+00\n",
      "Epoch 457/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0323 - accuracy: 0.0000e+00\n",
      "Epoch 457: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.0000e+00 - val_loss: 0.0165 - val_accuracy: 0.0000e+00\n",
      "Epoch 458/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00\n",
      "Epoch 458: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 459/500\n",
      "127/165 [======================>.......] - ETA: 0s - loss: 0.0391 - accuracy: 0.0000e+00\n",
      "Epoch 459: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 460/500\n",
      "158/165 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.0000e+00\n",
      "Epoch 460: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - val_loss: 0.0245 - val_accuracy: 0.0000e+00\n",
      "Epoch 461/500\n",
      "123/165 [=====================>........] - ETA: 0s - loss: 0.0228 - accuracy: 0.0000e+00\n",
      "Epoch 461: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - val_loss: 0.0179 - val_accuracy: 0.0000e+00\n",
      "Epoch 462/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.0000e+00\n",
      "Epoch 462: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.0000e+00 - val_loss: 0.0822 - val_accuracy: 0.0000e+00\n",
      "Epoch 463/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 2.6887 - accuracy: 0.0000e+00\n",
      "Epoch 463: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 2.2204 - accuracy: 0.0000e+00 - val_loss: 0.0221 - val_accuracy: 0.0000e+00\n",
      "Epoch 464/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.0000e+00\n",
      "Epoch 464: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1883 - accuracy: 0.0000e+00 - val_loss: 0.2064 - val_accuracy: 0.0000e+00\n",
      "Epoch 465/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 9.7646 - accuracy: 0.0000e+00 \n",
      "Epoch 465: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 9.0483 - accuracy: 0.0000e+00 - val_loss: 0.0206 - val_accuracy: 0.0000e+00\n",
      "Epoch 466/500\n",
      "131/165 [======================>.......] - ETA: 0s - loss: 0.0290 - accuracy: 0.0000e+00\n",
      "Epoch 466: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0000e+00 - val_loss: 0.0188 - val_accuracy: 0.0000e+00\n",
      "Epoch 467/500\n",
      "146/165 [=========================>....] - ETA: 0s - loss: 0.0183 - accuracy: 0.0000e+00\n",
      "Epoch 467: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - val_loss: 0.0259 - val_accuracy: 0.0000e+00\n",
      "Epoch 468/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.0626 - accuracy: 0.0000e+00\n",
      "Epoch 468: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.0000e+00 - val_loss: 0.0175 - val_accuracy: 0.0000e+00\n",
      "Epoch 469/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0165 - accuracy: 0.0000e+00\n",
      "Epoch 469: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - val_loss: 0.0586 - val_accuracy: 0.0000e+00\n",
      "Epoch 470/500\n",
      "141/165 [========================>.....] - ETA: 0s - loss: 0.2472 - accuracy: 0.0000e+00\n",
      "Epoch 470: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2160 - accuracy: 0.0000e+00 - val_loss: 0.0204 - val_accuracy: 0.0000e+00\n",
      "Epoch 471/500\n",
      "159/165 [===========================>..] - ETA: 0s - loss: 0.0178 - accuracy: 0.0000e+00\n",
      "Epoch 471: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - val_loss: 0.1133 - val_accuracy: 0.0000e+00\n",
      "Epoch 472/500\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.4861 - accuracy: 0.0000e+00\n",
      "Epoch 472: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 1.4861 - accuracy: 0.0000e+00 - val_loss: 0.0183 - val_accuracy: 0.0000e+00\n",
      "Epoch 473/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.0000e+00\n",
      "Epoch 473: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - val_loss: 0.0351 - val_accuracy: 0.0000e+00\n",
      "Epoch 474/500\n",
      "128/165 [======================>.......] - ETA: 0s - loss: 0.0356 - accuracy: 0.0000e+00\n",
      "Epoch 474: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 475/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.0000e+00\n",
      "Epoch 475: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 476/500\n",
      "153/165 [==========================>...] - ETA: 0s - loss: 0.0141 - accuracy: 0.0000e+00\n",
      "Epoch 476: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 477/500\n",
      "130/165 [======================>.......] - ETA: 0s - loss: 0.0276 - accuracy: 0.0000e+00\n",
      "Epoch 477: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - val_loss: 0.0158 - val_accuracy: 0.0000e+00\n",
      "Epoch 478/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.1778 - accuracy: 0.0000e+00\n",
      "Epoch 478: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1572 - accuracy: 0.0000e+00 - val_loss: 0.0314 - val_accuracy: 0.0000e+00\n",
      "Epoch 479/500\n",
      "135/165 [=======================>......] - ETA: 0s - loss: 0.0413 - accuracy: 0.0000e+00\n",
      "Epoch 479: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 480/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.0150 - accuracy: 0.0000e+00\n",
      "Epoch 480: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - val_loss: 0.1285 - val_accuracy: 0.0000e+00\n",
      "Epoch 481/500\n",
      "156/165 [===========================>..] - ETA: 0s - loss: 0.0840 - accuracy: 0.0000e+00\n",
      "Epoch 481: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.0000e+00 - val_loss: 0.1302 - val_accuracy: 0.0000e+00\n",
      "Epoch 482/500\n",
      "147/165 [=========================>....] - ETA: 0s - loss: 0.6878 - accuracy: 0.0000e+00\n",
      "Epoch 482: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.0000e+00 - val_loss: 0.0290 - val_accuracy: 0.0000e+00\n",
      "Epoch 483/500\n",
      "155/165 [===========================>..] - ETA: 0s - loss: 0.9570 - accuracy: 0.0000e+00\n",
      "Epoch 483: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9049 - accuracy: 0.0000e+00 - val_loss: 0.0160 - val_accuracy: 0.0000e+00\n",
      "Epoch 484/500\n",
      "145/165 [=========================>....] - ETA: 0s - loss: 0.0238 - accuracy: 0.0000e+00\n",
      "Epoch 484: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 485/500\n",
      "128/165 [======================>.......] - ETA: 0s - loss: 0.0137 - accuracy: 0.0000e+00\n",
      "Epoch 485: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.0000e+00 - val_loss: 0.0157 - val_accuracy: 0.0000e+00\n",
      "Epoch 486/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 0.1435 - accuracy: 0.0000e+00\n",
      "Epoch 486: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1404 - accuracy: 0.0000e+00 - val_loss: 0.0293 - val_accuracy: 0.0000e+00\n",
      "Epoch 487/500\n",
      "121/165 [=====================>........] - ETA: 0s - loss: 0.1328 - accuracy: 0.0000e+00\n",
      "Epoch 487: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.0000e+00 - val_loss: 0.0155 - val_accuracy: 0.0000e+00\n",
      "Epoch 488/500\n",
      "132/165 [=======================>......] - ETA: 0s - loss: 0.0200 - accuracy: 0.0000e+00\n",
      "Epoch 488: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
      "Epoch 489/500\n",
      "157/165 [===========================>..] - ETA: 0s - loss: 0.0194 - accuracy: 0.0000e+00\n",
      "Epoch 489: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n",
      "Epoch 490/500\n",
      "142/165 [========================>.....] - ETA: 0s - loss: 0.0144 - accuracy: 0.0000e+00\n",
      "Epoch 490: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.0000e+00 - val_loss: 0.0156 - val_accuracy: 0.0000e+00\n",
      "Epoch 491/500\n",
      "116/165 [====================>.........] - ETA: 0s - loss: 0.0138 - accuracy: 0.0000e+00\n",
      "Epoch 491: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - val_loss: 0.0239 - val_accuracy: 0.0000e+00\n",
      "Epoch 492/500\n",
      "123/165 [=====================>........] - ETA: 0s - loss: 0.0530 - accuracy: 0.0000e+00\n",
      "Epoch 492: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0541 - accuracy: 0.0000e+00 - val_loss: 0.0212 - val_accuracy: 0.0000e+00\n",
      "Epoch 493/500\n",
      "163/165 [============================>.] - ETA: 0s - loss: 0.7399 - accuracy: 0.0000e+00\n",
      "Epoch 493: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.7351 - accuracy: 0.0000e+00 - val_loss: 0.0178 - val_accuracy: 0.0000e+00\n",
      "Epoch 494/500\n",
      "128/165 [======================>.......] - ETA: 0s - loss: 0.0213 - accuracy: 0.0000e+00\n",
      "Epoch 494: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 495/500\n",
      "154/165 [===========================>..] - ETA: 0s - loss: 0.0157 - accuracy: 0.0000e+00\n",
      "Epoch 495: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.0000e+00 - val_loss: 0.0216 - val_accuracy: 0.0000e+00\n",
      "Epoch 496/500\n",
      "143/165 [=========================>....] - ETA: 0s - loss: 0.2862 - accuracy: 0.0000e+00\n",
      "Epoch 496: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.0000e+00 - val_loss: 0.7857 - val_accuracy: 0.0000e+00\n",
      "Epoch 497/500\n",
      "160/165 [============================>.] - ETA: 0s - loss: 1.0001 - accuracy: 0.0000e+00\n",
      "Epoch 497: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.9759 - accuracy: 0.0000e+00 - val_loss: 0.0181 - val_accuracy: 0.0000e+00\n",
      "Epoch 498/500\n",
      "149/165 [==========================>...] - ETA: 0s - loss: 0.4027 - accuracy: 0.0000e+00\n",
      "Epoch 498: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.0000e+00 - val_loss: 0.0292 - val_accuracy: 0.0000e+00\n",
      "Epoch 499/500\n",
      "152/165 [==========================>...] - ETA: 0s - loss: 0.0247 - accuracy: 0.0000e+00\n",
      "Epoch 499: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - val_loss: 0.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 500/500\n",
      "137/165 [=======================>......] - ETA: 0s - loss: 0.0196 - accuracy: 0.0000e+00\n",
      "Epoch 500: val_loss did not improve from 0.01520\n",
      "165/165 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - val_loss: 0.0153 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "trained_data = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, callbacks=model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengevaluasi model DNN dengan data test dan mengembalikan nilai kerugian (loss) dan akurasi yang diperoleh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/71 [..............................] - ETA: 2s - loss: 0.0152 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015279924497008324, 0.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengambil metrik akurasi (accuracy) dari setiap epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = trained_data.history['accuracy']\n",
    "validation_accuracy = trained_data.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat grafik untuk akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/0AAAHWCAYAAAAly+m8AAAAPHRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMHJjMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/RjVi6AAAACXBIWXMAAA9hAAAPYQGoP6dpAABXTElEQVR4nO3deVwW5f7/8fcNyCYCisjivnDcl8Il7JimGC5ZmJYSKi7lqVxTy0xzq5MtVmaWnjoulaKmKam5hLsZqam461HDXTQ1QVwAYX5/+PP+dgsKKOv0ej4e9+M411wz85nxOp7zvmfmui2GYRgCAAAAAACmY1fQBQAAAAAAgLxB6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAmEbPnj1VqVKl+9p27NixslgsuVtQIXPs2DFZLBbNmjUr349tsVg0duxY6/KsWbNksVh07NixLLetVKmSevbsmav1PMhYAQCgKCH0AwDynMViydZn/fr1BV3q397AgQNlsVh05MiRu/YZOXKkLBaLdu/enY+V5dyZM2c0duxYxcbGFnQpmTpw4IAsFoucnZ11+fLlgi4HAGBShH4AQJ779ttvbT6tW7fOtL1mzZoPdJyvvvpKhw4duq9tR40apevXrz/Q8c0gPDxckhQZGXnXPnPnzlXdunVVr169+z5O9+7ddf36dVWsWPG+95GVM2fOaNy4cZmG/gcZK7ll9uzZ8vX1lSQtXLiwQGsBAJiXQ0EXAAAwv27dutks//rrr4qOjs7Qfqdr167J1dU128cpVqzYfdUnSQ4ODnJw4H8WmzRpomrVqmnu3LkaPXp0hvUxMTGKi4vTe++990DHsbe3l729/QPt40E8yFjJDYZhKDIyUs8//7zi4uI0Z84cvfDCCwVa091cvXpVxYsXL+gyAAD3iTv9AIBCoUWLFqpTp462b9+uxx57TK6urnrzzTclST/88IPat28vf39/OTk5qWrVqnr77beVlpZms48739O+/Q77xIkT9eWXX6pq1apycnJSo0aNtG3bNpttM3un32KxqH///oqKilKdOnXk5OSk2rVra+XKlRnqX79+vRo2bChnZ2dVrVpV//nPf7I9T8CmTZv07LPPqkKFCnJyclL58uX16quvZnjyoGfPnnJzc9Pp06cVGhoqNzc3eXt7a9iwYRmuxeXLl9WzZ095eHjI09NTERER2X6EPDw8XAcPHtSOHTsyrIuMjJTFYlFYWJhSUlI0evRoBQYGysPDQ8WLF1ezZs20bt26LI+R2Tv9hmHonXfeUbly5eTq6qrHH39c+/bty7DtpUuXNGzYMNWtW1dubm5yd3dX27ZttWvXLmuf9evXq1GjRpKkXr16WV8huT2fQWbv9F+9elVDhw5V+fLl5eTkpOrVq2vixIkyDMOmX07Gxd1s3rxZx44dU9euXdW1a1dt3LhRp06dytAvPT1dn376qerWrStnZ2d5e3urTZs2+u2332z6zZ49W40bN5arq6tKliypxx57TD/99JNNzX+dU+G2O+dLuP33smHDBr3yyisqU6aMypUrJ0k6fvy4XnnlFVWvXl0uLi7y8vLSs88+m+m8DJcvX9arr76qSpUqycnJSeXKlVOPHj104cIFJSUlqXjx4ho0aFCG7U6dOiV7e3tNmDAhm1cSAJAVbmkAAAqNixcvqm3bturatau6desmHx8fSbeCiJubm4YMGSI3NzetXbtWo0ePVmJioj788MMs9xsZGakrV67oX//6lywWiz744AM988wz+v3337O84/vzzz9r0aJFeuWVV1SiRAlNnjxZnTp10okTJ+Tl5SVJ2rlzp9q0aSM/Pz+NGzdOaWlpGj9+vLy9vbN13gsWLNC1a9f08ssvy8vLS1u3btVnn32mU6dOacGCBTZ909LSFBISoiZNmmjixIlavXq1PvroI1WtWlUvv/yypFvh+emnn9bPP/+sl156STVr1tTixYsVERGRrXrCw8M1btw4RUZG6uGHH7Y59nfffadmzZqpQoUKunDhgv773/8qLCxML774oq5cuaLp06crJCREW7duVYMGDbJ1vNtGjx6td955R+3atVO7du20Y8cOPfHEE0pJSbHp9/vvvysqKkrPPvusKleurHPnzuk///mPmjdvrv3798vf3181a9bU+PHjNXr0aPXt21fNmjWTJDVt2jTTYxuGoaeeekrr1q1Tnz591KBBA61atUqvvfaaTp8+rU8++cSmf3bGxb3MmTNHVatWVaNGjVSnTh25urpq7ty5eu2112z69enTR7NmzVLbtm31wgsv6ObNm9q0aZN+/fVXNWzYUJI0btw4jR07Vk2bNtX48ePl6OioLVu2aO3atXriiSeyff3/6pVXXpG3t7dGjx6tq1evSpK2bdumX375RV27dlW5cuV07NgxTZ06VS1atND+/futT+UkJSWpWbNmOnDggHr37q2HH35YFy5c0JIlS3Tq1Ck1aNBAHTt21Pz58/Xxxx/bPPExd+5cGYZhfc0EAJALDAAA8lm/fv2MO/8nqHnz5oYkY9q0aRn6X7t2LUPbv/71L8PV1dW4ceOGtS0iIsKoWLGidTkuLs6QZHh5eRmXLl2ytv/www+GJGPp0qXWtjFjxmSoSZLh6OhoHDlyxNq2a9cuQ5Lx2WefWds6dOhguLq6GqdPn7a2HT582HBwcMiwz8xkdn4TJkwwLBaLcfz4cZvzk2SMHz/epu9DDz1kBAYGWpejoqIMScYHH3xgbbt586bRrFkzQ5Ixc+bMLGtq1KiRUa5cOSMtLc3atnLlSkOS8Z///Me6z+TkZJvt/vzzT8PHx8fo3bu3TbskY8yYMdblmTNnGpKMuLg4wzAM4/z584ajo6PRvn17Iz093drvzTffNCQZERER1rYbN27Y1GUYt/6unZycbK7Ntm3b7nq+d46V29fsnXfesenXuXNnw2Kx2IyB7I6Lu0lJSTG8vLyMkSNHWtuef/55o379+jb91q5da0gyBg4cmGEft6/R4cOHDTs7O6Njx44Zrslfr+Od1/+2ihUr2lzb238v//znP42bN2/a9M1snMbExBiSjG+++cbaNnr0aEOSsWjRorvWvWrVKkOSsWLFCpv19erVM5o3b55hOwDA/ePxfgBAoeHk5KRevXplaHdxcbH++cqVK7pw4YKaNWuma9eu6eDBg1nut0uXLipZsqR1+fZd399//z3LbYODg1W1alXrcr169eTu7m7dNi0tTatXr1ZoaKj8/f2t/apVq6a2bdtmuX/J9vyuXr2qCxcuqGnTpjIMQzt37szQ/6WXXrJZbtasmc25LF++XA4ODtY7/9Ktd+gHDBiQrXqkW/MwnDp1Shs3brS2RUZGytHRUc8++6x1n46OjpJuPYZ+6dIl3bx5Uw0bNsz01YB7Wb16tVJSUjRgwACbVyIGDx6coa+Tk5Ps7G79X5i0tDRdvHhRbm5uql69eo6Pe9vy5ctlb2+vgQMH2rQPHTpUhmFoxYoVNu1ZjYt7WbFihS5evKiwsDBrW1hYmHbt2mXzOsP3338vi8WiMWPGZNjH7WsUFRWl9PR0jR492npN7uxzP1588cUMcy78dZympqbq4sWLqlatmjw9PW2u+/fff6/69eurY8eOd607ODhY/v7+mjNnjnXd3r17tXv37izn+gAA5AyhHwBQaJQtW9YaIv9q37596tixozw8POTu7i5vb29rMEhISMhyvxUqVLBZvv0FwJ9//pnjbW9vf3vb8+fP6/r166pWrVqGfpm1ZebEiRPq2bOnSpUqZX1Pv3nz5pIynt/t97rvVo90691rPz8/ubm52fSrXr16tuqRpK5du8re3t46i/+NGze0ePFitW3b1uYLlK+//lr16tWTs7OzvLy85O3trR9//DFbfy9/dfz4cUlSQECATbu3t7fN8aRbXzB88sknCggIkJOTk0qXLi1vb2/t3r07x8f96/H9/f1VokQJm/bbvyhxu77bshoX9zJ79mxVrlxZTk5OOnLkiI4cOaKqVavK1dXVJgQfPXpU/v7+KlWq1F33dfToUdnZ2alWrVpZHjcnKleunKHt+vXrGj16tHXOg9vX/fLlyzbX/ejRo6pTp849929nZ6fw8HBFRUXp2rVrkm698uDs7Gz9UgkAkDsI/QCAQuOvdxJvu3z5spo3b65du3Zp/PjxWrp0qaKjo/X+++9LuhUAs3K3WeKNOyZoy+1tsyMtLU2tW7fWjz/+qOHDhysqKkrR0dHWCefuPL/8mvG+TJkyat26tb7//nulpqZq6dKlunLlis271rNnz1bPnj1VtWpVTZ8+XStXrlR0dLRatmyZrb+X+/Xuu+9qyJAheuyxxzR79mytWrVK0dHRql27dp4e96/ud1wkJiZq6dKliouLU0BAgPVTq1YtXbt2TZGRkbk2trLjzgkgb8vsv4sDBgzQv//9bz333HP67rvv9NNPPyk6OlpeXl73dd179OihpKQkRUVFWX/N4Mknn5SHh0eO9wUAuDsm8gMAFGrr16/XxYsXtWjRIj322GPW9ri4uAKs6v+UKVNGzs7OOnLkSIZ1mbXdac+ePfrf//6nr7/+Wj169LC2R0dH33dNFStW1Jo1a5SUlGRztz+nv0sfHh6ulStXasWKFYqMjJS7u7s6dOhgXb9w4UJVqVJFixYtsnmUPLPH0bNTsyQdPnxYVapUsbb/8ccfGe6eL1y4UI8//rimT59u03758mWVLl3aupyTx9srVqyo1atX68qVKzZ3+2+/PnK7vge1aNEi3bhxQ1OnTrWpVbr19zNq1Cht3rxZ//znP1W1alWtWrVKly5duuvd/qpVqyo9PV379++/58SJJUuWzPDrDSkpKTp79my2a1+4cKEiIiL00UcfWdtu3LiRYb9Vq1bV3r17s9xfnTp19NBDD2nOnDkqV66cTpw4oc8++yzb9QAAsoc7/QCAQu32HdW/3v1MSUnRF198UVAl2bC3t1dwcLCioqJ05swZa/uRI0cyvAd+t+0l2/MzDEOffvrpfdfUrl073bx5U1OnTrW2paWl5ThQhYaGytXVVV988YVWrFihZ555Rs7OzvesfcuWLYqJiclxzcHBwSpWrJg+++wzm/1NmjQpQ197e/sMd8MXLFig06dP27Td/m357PxUYbt27ZSWlqYpU6bYtH/yySeyWCzZnp8hK7Nnz1aVKlX00ksvqXPnzjafYcOGyc3NzfqIf6dOnWQYhsaNG5dhP7fPPzQ0VHZ2dho/fnyGu+1/vUZVq1a1mZ9Bkr788su73unPTGbX/bPPPsuwj06dOmnXrl1avHjxXeu+rXv37vrpp580adIkeXl55dp1BgD8H+70AwAKtaZNm6pkyZKKiIjQwIEDZbFY9O233+brI9BZGTt2rH766Sc9+uijevnll63hsU6dOoqNjb3ntjVq1FDVqlU1bNgwnT59Wu7u7vr++++z9W743XTo0EGPPvqo3njjDR07dky1atXSokWLcvy+u5ubm0JDQ63v9d/5M2pPPvmkFi1apI4dO6p9+/aKi4vTtGnTVKtWLSUlJeXoWN7e3ho2bJgmTJigJ598Uu3atdPOnTu1YsWKDHfEn3zySY0fP169evVS06ZNtWfPHs2ZM8fmCQHpVtD19PTUtGnTVKJECRUvXlxNmjTJ9H31Dh066PHHH9fIkSN17Ngx1a9fXz/99JN++OEHDR482GbSvvt15swZrVu3LsNkgbc5OTkpJCRECxYs0OTJk/X444+re/fumjx5sg4fPqw2bdooPT1dmzZt0uOPP67+/furWrVqGjlypN5++201a9ZMzzzzjJycnLRt2zb5+/tbf+/+hRde0EsvvaROnTqpdevW2rVrl1atWpXh2t7Lk08+qW+//VYeHh6qVauWYmJitHr16gw/Ufjaa69p4cKFevbZZ9W7d28FBgbq0qVLWrJkiaZNm6b69etb+z7//PN6/fXXtXjxYr388stZ/oQmACDnuNMPACjUvLy8tGzZMvn5+WnUqFGaOHGiWrdurQ8++KCgS7MKDAzUihUrVLJkSb311luaPn26xo8fr1atWtncGc9MsWLFtHTpUjVo0EATJkzQuHHjFBAQoG+++ea+67Gzs9OSJUsUHh6u2bNna+TIkSpbtqy+/vrrHO/rdtD38/NTy5Ytbdb17NlT7777rnbt2qWBAwdq1apVmj17tvX343PqnXfe0bhx47Rz50699tprOnr0qH766SfrHfvb3nzzTQ0dOlSrVq3SoEGDtGPHDv34448qX768Tb9ixYrp66+/lr29vV566SWFhYVpw4YNmR779jUbPHiwli1bpsGDB2v//v368MMP9fHHH9/X+dxp3rx5Sk9Pt3lF4k4dOnTQxYsXrU+JzJw5Ux9++KHi4uL02muv6d1339X169fVtGlT6zbjx4/XjBkzdP36dY0cOVKjR4/W8ePH1apVK2ufF198UcOHD9fGjRs1dOhQxcXFKTo6OsO1vZdPP/1UPXr00Jw5czR06FCdPXtWq1evzjBhpJubmzZt2qSXX35Zy5cv18CBA/XFF1+oevXqKleunE1fHx8fPfHEE5Ju3fUHAOQ+i1GYbpUAAGAioaGh2rdvnw4fPlzQpQCFVseOHbVnz55szYEBAMg57vQDAJALrl+/brN8+PBhLV++XC1atCiYgoAi4OzZs/rxxx+5yw8AeYg7/QAA5AI/Pz/17NlTVapU0fHjxzV16lQlJydr586dGX57Hvi7i4uL0+bNm/Xf//5X27Zt09GjR+Xr61vQZQGAKTGRHwAAuaBNmzaaO3eu4uPj5eTkpKCgIL377rsEfiATGzZsUK9evVShQgV9/fXXBH4AyEPc6QcAAAAAwKR4px8AAAAAAJMi9AMAAAAAYFK8058L0tPTdebMGZUoUUIWi6WgywEAAAAAmJxhGLpy5Yr8/f1lZ3f3+/mE/lxw5swZlS9fvqDLAAAAAAD8zZw8eVLlypW763pCfy4oUaKEpFsX293dvYCrAQAAAACYXWJiosqXL2/No3dD6M8Ftx/pd3d3J/QDAAAAAPJNVq+YM5EfAAAAAAAmRegHAAAAAMCkCP0AAAAAAJgU7/QDAAAAKJIMw9DNmzeVlpZW0KUAuc7e3l4ODg4P/LPwhH4AAAAARU5KSorOnj2ra9euFXQpQJ5xdXWVn5+fHB0d73sfhH4AAAAARUp6erri4uJkb28vf39/OTo6PvDdUKAwMQxDKSkp+uOPPxQXF6eAgADZ2d3f2/mEfgAAAABFSkpKitLT01W+fHm5uroWdDlAnnBxcVGxYsV0/PhxpaSkyNnZ+b72w0R+AAAAAIqk+73zCRQVuTHG+W8JAAAAAAAmRegHAAAAAMCkCP0AAAAAUERVqlRJkyZNynb/9evXy2Kx6PLly3lWEwoXQj8AAAAA5DGLxXLPz9ixY+9rv9u2bVPfvn2z3b9p06Y6e/asPDw87ut496NGjRpycnJSfHx8vh0T/4fQDwAAAAB57OzZs9bPpEmT5O7ubtM2bNgwa1/DMHTz5s1s7dfb2ztHv2Dg6OgoX1/ffPuJw59//lnXr19X586d9fXXX+fLMe8lNTW1oEvId4R+AAAAAEWaYUhXrxbMxzCyV6Ovr6/14+HhIYvFYl0+ePCgSpQooRUrVigwMFBOTk76+eefdfToUT399NPy8fGRm5ubGjVqpNWrV9vs987H+y0Wi/773/+qY8eOcnV1VUBAgJYsWWJdf+fj/bNmzZKnp6dWrVqlmjVrys3NTW3atNHZs2et29y8eVMDBw6Up6envLy8NHz4cEVERCg0NDTL854+fbqef/55de/eXTNmzMiw/tSpUwoLC1OpUqVUvHhxNWzYUFu2bLGuX7p0qRo1aiRnZ2eVLl1aHTt2tDnXqKgom/15enpq1qxZkqRjx47JYrFo/vz5at68uZydnTVnzhxdvHhRYWFhKlu2rFxdXVW3bl3NnTvXZj/p6en64IMPVK1aNTk5OalChQr697//LUlq2bKl+vfvb9P/jz/+kKOjo9asWZPlNclvhH4AAAAARdq1a5KbW8F8rl3LvfN444039N577+nAgQOqV6+ekpKS1K5dO61Zs0Y7d+5UmzZt1KFDB504ceKe+xk3bpyee+457d69W+3atVN4eLguXbp0j+t3TRMnTtS3336rjRs36sSJEzZPHrz//vuaM2eOZs6cqc2bNysxMTFD2M7MlStXtGDBAnXr1k2tW7dWQkKCNm3aZF2flJSk5s2b6/Tp01qyZIl27dql119/Xenp6ZKkH3/8UR07dlS7du20c+dOrVmzRo0bN87yuHd64403NGjQIB04cEAhISG6ceOGAgMD9eOPP2rv3r3q27evunfvrq1bt1q3GTFihN577z299dZb2r9/vyIjI+Xj4yNJeuGFFxQZGank5GRr/9mzZ6ts2bJq2bJljuvLcwYeWEJCgiHJSEhIKOhSAAAAANO7fv26sX//fuP69euGYRhGUpJh3Lrnnv+fpKSc1z9z5kzDw8PDurxu3TpDkhEVFZXltrVr1zY+++wz63LFihWNTz75xLosyRg1apR1OSkpyZBkrFixwuZYf/75p7UWScaRI0es23z++eeGj4+PddnHx8f48MMPrcs3b940KlSoYDz99NP3rPXLL780GjRoYF0eNGiQERERYV3+z3/+Y5QoUcK4ePFiptsHBQUZ4eHhd92/JGPx4sU2bR4eHsbMmTMNwzCMuLg4Q5IxadKke9ZpGIbRvn17Y+jQoYZhGEZiYqLh5ORkfPXVV5n2vX79ulGyZElj/vz51rZ69eoZY8eOzfI4OXXnWP+r7OZQhwL7tgEAAAAAcoGrq5SUVHDHzi0NGza0WU5KStLYsWP1448/6uzZs7p586auX7+e5Z3+evXqWf9cvHhxubu76/z583ft7+rqqqpVq1qX/fz8rP0TEhJ07tw5mzvs9vb2CgwMtN6Rv5sZM2aoW7du1uVu3bqpefPm+uyzz1SiRAnFxsbqoYceUqlSpTLdPjY2Vi+++OI9j5Edd17XtLQ0vfvuu/ruu+90+vRppaSkKDk52To3woEDB5ScnKxWrVpluj9nZ2fr6wrPPfecduzYob1799q8RlGYEPoBAAAAFGkWi1S8eEFX8eCK33ESw4YNU3R0tCZOnKhq1arJxcVFnTt3VkpKyj33U6xYMZtli8Vyz4CeWX8ju5MV3MX+/fv166+/auvWrRo+fLi1PS0tTfPmzdOLL74oFxeXe+4jq/WZ1ZnZRH13XtcPP/xQn376qSZNmqS6deuqePHiGjx4sPW6ZnVc6dYj/g0aNNCpU6c0c+ZMtWzZUhUrVsxyu4LAO/0AAAAAUAht3rxZPXv2VMeOHVW3bl35+vrq2LFj+VqDh4eHfHx8tG3bNmtbWlqaduzYcc/tpk+frscee0y7du1SbGys9TNkyBBNnz5d0q0nEmJjY+8630C9evXuOTGet7e3zYSDhw8f1rVsTLKwefNmPf300+rWrZvq16+vKlWq6H//+591fUBAgFxcXO557Lp166phw4b66quvFBkZqd69e2d53IJC6AcAAACAQiggIECLFi1SbGysdu3apeeffz7LR+rzwoABAzRhwgT98MMPOnTokAYNGqQ///zzrj/7l5qaqm+//VZhYWGqU6eOzeeFF17Qli1btG/fPoWFhcnX11ehoaHavHmzfv/9d33//feKiYmRJI0ZM0Zz587VmDFjdODAAe3Zs0fvv/++9TgtW7bUlClTtHPnTv3222966aWXMjy1kJmAgABFR0frl19+0YEDB/Svf/1L586ds653dnbW8OHD9frrr+ubb77R0aNH9euvv1q/rLjthRde0HvvvSfDMGx+VaCwIfQDAAAAQCH08ccfq2TJkmratKk6dOigkJAQPfzww/lex/DhwxUWFqYePXooKChIbm5uCgkJkbOzc6b9lyxZoosXL2YahGvWrKmaNWtq+vTpcnR01E8//aQyZcqoXbt2qlu3rt577z3Z29tLklq0aKEFCxZoyZIlatCggVq2bGkzw/5HH32k8uXLq1mzZnr++ec1bNgw63v59zJq1Cg9/PDDCgkJUYsWLaxfPPzVW2+9paFDh2r06NGqWbOmunTpkmFehLCwMDk4OCgsLOyu16IwsBgP+rIGlJiYKA8PDyUkJMjd3b2gywEAAABM7caNG4qLi1PlypULddgyq/T0dNWsWVPPPfec3n777YIup8AcO3ZMVatW1bZt2/Lsy5h7jfXs5lAm8gMAAAAA3NXx48f1008/qXnz5kpOTtaUKVMUFxen559/vqBLKxCpqam6ePGiRo0apUceeaRAnr7ICR7vBwAAAADclZ2dnWbNmqVGjRrp0Ucf1Z49e7R69WrVrFmzoEsrEJs3b5afn5+2bdumadOmFXQ5WeJOPwAAAADgrsqXL6/NmzcXdBmFRosWLR74Jw3zE3f6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAoIlq0aKHBgwdblytVqqRJkybdcxuLxaKoqKgHPnZu7Qf5i9APAAAAAHmsQ4cOatOmTabrNm3aJIvFot27d+d4v9u2bVPfvn0ftDwbY8eOVYMGDTK0nz17Vm3bts3VY93N9evXVapUKZUuXVrJycn5ckyzIvQDAAAAQB7r06ePoqOjderUqQzrZs6cqYYNG6pevXo53q+3t7dcXV1zo8Qs+fr6ysnJKV+O9f3336t27dqqUaNGgT9dYBiGbt68WaA1PAhCPwAAAICizTCkm1cL5mMY2SrxySeflLe3t2bNmmXTnpSUpAULFqhPnz66ePGiwsLCVLZsWbm6uqpu3bqaO3fuPfd75+P9hw8f1mOPPSZnZ2fVqlVL0dHRGbYZPny4/vGPf8jV1VVVqlTRW2+9pdTUVEnSrFmzNG7cOO3atUsWi0UWi8Va852P9+/Zs0ctW7aUi4uLvLy81LdvXyUlJVnX9+zZU6GhoZo4caL8/Pzk5eWlfv36WY91L9OnT1e3bt3UrVs3TZ8+PcP6ffv26cknn5S7u7tKlCihZs2a6ejRo9b1M2bMUO3ateXk5CQ/Pz/1799fknTs2DFZLBbFxsZa+16+fFkWi0Xr16+XJK1fv14Wi0UrVqxQYGCgnJyc9PPPP+vo0aN6+umn5ePjIzc3NzVq1EirV6+2qSs5OVnDhw9X+fLl5eTkpGrVqmn69OkyDEPVqlXTxIkTbfrHxsbKYrHoyJEjWV6T++WQZ3sGAAAAgPyQdk36zq1gjv1ckuRQPMtuDg4O6tGjh2bNmqWRI0fKYrFIkhYsWKC0tDSFhYUpKSlJgYGBGj58uNzd3fXjjz+qe/fuqlq1qho3bpzlMdLT0/XMM8/Ix8dHW7ZsUUJCgs37/7eVKFFCs2bNkr+/v/bs2aMXX3xRJUqU0Ouvv64uXbpo7969WrlypTXQenh4ZNjH1atXFRISoqCgIG3btk3nz5/XCy+8oP79+9t8sbFu3Tr5+flp3bp1OnLkiLp06aIGDRroxRdfvOt5HD16VDExMVq0aJEMw9Crr76q48ePq2LFipKk06dP67HHHlOLFi20du1aubu7a/Pmzda78VOnTtWQIUP03nvvqW3btkpISNDmzZuzvH53euONNzRx4kRVqVJFJUuW1MmTJ9WuXTv9+9//lpOTk7755ht16NBBhw4dUoUKFSRJPXr0UExMjCZPnqz69esrLi5OFy5ckMViUe/evTVz5kwNGzbMeoyZM2fqscceU7Vq1XJcX3YR+gEAAAAgH/Tu3VsffvihNmzYoBYtWki6Ffo6deokDw8PeXh42ATCAQMGaNWqVfruu++yFfpXr16tgwcPatWqVfL395ckvfvuuxnewx81apT1z5UqVdKwYcM0b948vf7663JxcZGbm5scHBzk6+t712NFRkbqxo0b+uabb1S8+K0vPaZMmaIOHTro/fffl4+PjySpZMmSmjJliuzt7VWjRg21b99ea9asuWfonzFjhtq2bauSJUtKkkJCQjRz5kyNHTtWkvT555/Lw8ND8+bNU7FixSRJ//jHP6zbv/POOxo6dKgGDRpkbWvUqFGW1+9O48ePV+vWra3LpUqVUv369a3Lb7/9thYvXqwlS5aof//++t///qfvvvtO0dHRCg4OliRVqVLF2r9nz54aPXq0tm7dqsaNGys1NVWRkZEZ7v7nNkI/AAAAgKLN3vXWHfeCOnY21ahRQ02bNtWMGTPUokULHTlyRJs2bdL48eMlSWlpaXr33Xf13Xff6fTp00pJSVFycnK239k/cOCAypcvbw38khQUFJSh3/z58zV58mQdPXpUSUlJunnzptzd3bN9HrePVb9+fWvgl6RHH31U6enpOnTokDX0165dW/b29tY+fn5+2rNnz133m5aWpq+//lqffvqpta1bt24aNmyYRo8eLTs7O8XGxqpZs2bWwP9X58+f15kzZ9SqVascnU9mGjZsaLOclJSksWPH6scff9TZs2d18+ZNXb9+XSdOnJB061F9e3t7NW/ePNP9+fv7q3379poxY4YaN26spUuXKjk5Wc8+++wD13ovvNMPAAAAoGizWG49Yl8Qn///mH529enTR99//72uXLmimTNnqmrVqtaQ+OGHH+rTTz/V8OHDtW7dOsXGxiokJEQpKSm5dqliYmIUHh6udu3aadmyZdq5c6dGjhyZq8f4qzuDucViUXp6+l37r1q1SqdPn1aXLl3k4OAgBwcHde3aVcePH9eaNWskSS4uLnfd/l7rJMnO7lYENv4yF8Pd5hj46xcakjRs2DAtXrxY7777rjZt2qTY2FjVrVvXeu2yOrYkvfDCC5o3b56uX7+umTNnqkuXLnk+ESOhHwAAAADyyXPPPSc7OztFRkbqm2++Ue/eva3v92/evFlPP/20unXrpvr166tKlSr63//+l+1916xZUydPntTZs2etbb/++qtNn19++UUVK1bUyJEj1bBhQwUEBOj48eM2fRwdHZWWlpblsXbt2qWrV69a2zZv3iw7OztVr1492zXfafr06eratatiY2NtPl27drVO6FevXj1t2rQp07BeokQJVapUyfoFwZ28vb0lyeYa/XVSv3vZvHmzevbsqY4dO6pu3bry9fXVsWPHrOvr1q2r9PR0bdiw4a77aNeunYoXL66pU6dq5cqV6t27d7aO/SAI/QAAAACQT9zc3NSlSxeNGDFCZ8+eVc+ePa3rAgICFB0drV9++UUHDhzQv/71L507dy7b+w4ODtY//vEPRUREaNeuXdq0aZNGjhxp0ycgIEAnTpzQvHnzdPToUU2ePFmLFy+26VOpUiXFxcUpNjZWFy5cUHJycoZjhYeHy9nZWREREdq7d6/WrVunAQMGqHv37tZH+3Pqjz/+0NKlSxUREaE6derYfHr06KGoqChdunRJ/fv3V2Jiorp27arffvtNhw8f1rfffqtDhw5JksaOHauPPvpIkydP1uHDh7Vjxw599tlnkm7djX/kkUf03nvv6cCBA9qwYYPNHAf3EhAQoEWLFik2Nla7du3S888/b/PUQqVKlRQREaHevXsrKipKcXFxWr9+vb777jtrH3t7e/Xs2VMjRoxQQEBApq9f5DZCPwAAAADkoz59+ujPP/9USEiIzfv3o0aN0sMPP6yQkBC1aNFCvr6+Cg0NzfZ+7ezstHjxYl2/fl2NGzfWCy+8oH//+982fZ566im9+uqr6t+/vxo0aKBffvlFb731lk2fTp06qU2bNnr88cfl7e2d6c8Gurq6atWqVbp06ZIaNWqkzp07q1WrVpoyZUrOLsZf3J4UMLP38Vu1aiUXFxfNnj1bXl5eWrt2rZKSktS8eXMFBgbqq6++sr5KEBERoUmTJumLL75Q7dq19eSTT+rw4cPWfc2YMUM3b95UYGCgBg8erHfeeSdb9X388ccqWbKkmjZtqg4dOigkJEQPP/ywTZ+pU6eqc+fOeuWVV1SjRg29+OKLNk9DSLf+/lNSUtSrV6+cXqL7YjGMbP6wJO4qMTFRHh4eSkhIyPEEGAAAAABy5saNG4qLi1PlypXl7Oxc0OUAObJp0ya1atVKJ0+ezPKpiHuN9ezmUGbvBwAAAAAgjyUnJ+uPP/7Q2LFj9eyzz973axA5xeP9AAAAAADksblz56pixYq6fPmyPvjgg3w7bpEL/Z9//rkqVaokZ2dnNWnSRFu3br1n/wULFqhGjRpydnZW3bp1tXz58rv2femll2SxWDRp0qRcrhoAAAAA8HfWs2dPpaWlafv27Spbtmy+HbdIhf758+dryJAhGjNmjHbs2KH69esrJCRE58+fz7T/L7/8orCwMPXp00c7d+5UaGioQkNDtXfv3gx9Fy9erF9//dVmIg0AAAAAAIqyIhX6P/74Y7344ovq1auXatWqpWnTpsnV1VUzZszItP+nn36qNm3a6LXXXlPNmjX19ttv6+GHH84wo+Tp06c1YMAAzZkzxzrjIwAAAIDCjTnJYXa5McaLTOhPSUnR9u3bFRwcbG2zs7NTcHCwYmJiMt0mJibGpr8khYSE2PRPT09X9+7d9dprr6l27drZqiU5OVmJiYk2HwAAAAD54/aNumvXrhVwJUDeuj3GH+TmdJGZvf/ChQtKS0vLMMOhj4+PDh48mOk28fHxmfaPj4+3Lr///vtycHDQwIEDs13LhAkTNG7cuBxUDwAAACC32Nvby9PT0/qar6urqywWSwFXBeQewzB07do1nT9/Xp6enrK3t7/vfRWZ0J8Xtm/frk8//VQ7duzI0T8SI0aM0JAhQ6zLiYmJKl++fF6UCAAAACATvr6+knTX+b0AM/D09LSO9ftVZEJ/6dKlZW9vr3Pnztm0nzt37q4XwdfX9579N23apPPnz6tChQrW9WlpaRo6dKgmTZqkY8eOZbpfJycnOTk5PcDZAAAAAHgQFotFfn5+KlOmjFJTUwu6HCDXFStW7IHu8N9WZEK/o6OjAgMDtWbNGoWGhkq69T7+mjVr1L9//0y3CQoK0po1azR48GBrW3R0tIKCgiRJ3bt3z/Sd/+7du6tXr155ch4AAAAAco+9vX2uBCPArIpM6JekIUOGKCIiQg0bNlTjxo01adIkXb161RrQe/ToobJly2rChAmSpEGDBql58+b66KOP1L59e82bN0+//fabvvzyS0mSl5eXvLy8bI5RrFgx+fr6qnr16vl7cgAAAAAA5LIiFfq7dOmiP/74Q6NHj1Z8fLwaNGiglStXWifrO3HihOzs/u8HCZo2barIyEiNGjVKb775pgICAhQVFaU6deoU1CkAAAAAAJBvLAY/bvnAEhMT5eHhoYSEBLm7uxd0OQAAAAAAk8tuDrW76xoAAAAAAFCkEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZV5EL/559/rkqVKsnZ2VlNmjTR1q1b79l/wYIFqlGjhpydnVW3bl0tX77cui41NVXDhw9X3bp1Vbx4cfn7+6tHjx46c+ZMXp8GAAAAAAB5rkiF/vnz52vIkCEaM2aMduzYofr16yskJETnz5/PtP8vv/yisLAw9enTRzt37lRoaKhCQ0O1d+9eSdK1a9e0Y8cOvfXWW9qxY4cWLVqkQ4cO6amnnsrP0wIAAAAAIE9YDMMwCrqI7GrSpIkaNWqkKVOmSJLS09NVvnx5DRgwQG+88UaG/l26dNHVq1e1bNkya9sjjzyiBg0aaNq0aZkeY9u2bWrcuLGOHz+uChUqZKuuxMREeXh4KCEhQe7u7vdxZgAAAAAAZF92c2iRudOfkpKi7du3Kzg42NpmZ2en4OBgxcTEZLpNTEyMTX9JCgkJuWt/SUpISJDFYpGnp+dd+yQnJysxMdHmAwAAAABAYVNkQv+FCxeUlpYmHx8fm3YfHx/Fx8dnuk18fHyO+t+4cUPDhw9XWFjYPb8pmTBhgjw8PKyf8uXL5/BsAAAAAADIe0Um9Oe11NRUPffcczIMQ1OnTr1n3xEjRighIcH6OXnyZD5VCQAAAABA9jkUdAHZVbp0adnb2+vcuXM27efOnZOvr2+m2/j6+mar/+3Af/z4ca1duzbL9/KdnJzk5OR0H2cBAAAAAED+KTJ3+h0dHRUYGKg1a9ZY29LT07VmzRoFBQVluk1QUJBNf0mKjo626X878B8+fFirV6+Wl5dX3pwAAAAAAAD5rMjc6ZekIUOGKCIiQg0bNlTjxo01adIkXb16Vb169ZIk9ejRQ2XLltWECRMkSYMGDVLz5s310UcfqX379po3b55+++03ffnll5JuBf7OnTtrx44dWrZsmdLS0qzv+5cqVUqOjo4Fc6IAAAAAAOSCIhX6u3Tpoj/++EOjR49WfHy8GjRooJUrV1on6ztx4oTs7P7v4YWmTZsqMjJSo0aN0ptvvqmAgABFRUWpTp06kqTTp09ryZIlkqQGDRrYHGvdunVq0aJFvpwXAAAAAAB5wWIYhlHQRRR12f19RAAAAAAAckN2c2iReacfAAAAAADkDKEfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAAAAAGBSOQ79lSpV0vjx43XixIm8qAcAAAAAAOSSHIf+wYMHa9GiRapSpYpat26tefPmKTk5OS9qAwAAAAAAD+C+Qn9sbKy2bt2qmjVrasCAAfLz81P//v21Y8eOvKgRAAAAAADcB4thGMaD7CA1NVVffPGFhg8frtTUVNWtW1cDBw5Ur169ZLFYcqvOQi0xMVEeHh5KSEiQu7t7QZcDAAAAADC57OZQh/s9QGpqqhYvXqyZM2cqOjpajzzyiPr06aNTp07pzTff1OrVqxUZGXm/uwcAAAAAAA8ox6F/x44dmjlzpubOnSs7Ozv16NFDn3zyiWrUqGHt07FjRzVq1ChXCwUAAAAAADmT49DfqFEjtW7dWlOnTlVoaKiKFSuWoU/lypXVtWvXXCkQAAAAAADcnxyH/t9//10VK1a8Z5/ixYtr5syZ910UAAAAAAB4cDmevf/8+fPasmVLhvYtW7bot99+y5WiAAAAAADAg8tx6O/Xr59OnjyZof306dPq169frhQFAAAAAAAeXI5D//79+/Xwww9naH/ooYe0f//+XCkKAAAAAAA8uByHficnJ507dy5D+9mzZ+XgcN+/AAgAAAAAAHJZjkP/E088oREjRighIcHadvnyZb355ptq3bp1rhYHAAAAAADuX45vzU+cOFGPPfaYKlasqIceekiSFBsbKx8fH3377be5XiAAAAAAALg/OQ79ZcuW1e7duzVnzhzt2rVLLi4u6tWrl8LCwlSsWLG8qBEAAAAAANyH+3oJv3jx4urbt29u1wIAAAAAAHLRfc+8t3//fp04cUIpKSk27U899dQDFwUAAAAAAB5cjkP/77//ro4dO2rPnj2yWCwyDEOSZLFYJElpaWm5WyEAAAAAALgvOZ69f9CgQapcubLOnz8vV1dX7du3Txs3blTDhg21fv36PCgRAAAAAADcjxzf6Y+JidHatWtVunRp2dnZyc7OTv/85z81YcIEDRw4UDt37syLOgEAAAAAQA7l+E5/WlqaSpQoIUkqXbq0zpw5I0mqWLGiDh06lLvVAQAAAACA+5bjO/116tTRrl27VLlyZTVp0kQffPCBHB0d9eWXX6pKlSp5USMAAAAAALgPOQ79o0aN0tWrVyVJ48eP15NPPqlmzZrJy8tL8+fPz/UCAQAAAADA/bEYt6fffwCXLl1SyZIlrTP4/90kJibKw8NDCQkJcnd3L+hyAAAAAAAml90cmqN3+lNTU+Xg4KC9e/fatJcqVSrfAv/nn3+uSpUqydnZWU2aNNHWrVvv2X/BggWqUaOGnJ2dVbduXS1fvtxmvWEYGj16tPz8/OTi4qLg4GAdPnw4L08BAAAAAIB8kaPQX6xYMVWoUEFpaWl5Vc89zZ8/X0OGDNGYMWO0Y8cO1a9fXyEhITp//nym/X/55ReFhYWpT58+2rlzp0JDQxUaGmrzpcUHH3ygyZMna9q0adqyZYuKFy+ukJAQ3bhxI79OCwAAAACAPJHjx/unT5+uRYsW6dtvv1WpUqXyqq5MNWnSRI0aNdKUKVMkSenp6SpfvrwGDBigN954I0P/Ll266OrVq1q2bJm17ZFHHlGDBg00bdo0GYYhf39/DR06VMOGDZMkJSQkyMfHR7NmzVLXrl2zVVdReLzfSDd07cq1gi4DAAAAAAo91xKustgV7tfXs5tDczyR35QpU3TkyBH5+/urYsWKKl68uM36HTt25LzabEhJSdH27ds1YsQIa5udnZ2Cg4MVExOT6TYxMTEaMmSITVtISIiioqIkSXFxcYqPj1dwcLB1vYeHh5o0aaKYmJi7hv7k5GQlJydblxMTE+/3tPLNtSvXVPxHt4IuAwAAAAAKvavtk1Tco3jWHYuAHIf+0NDQPCgjaxcuXFBaWpp8fHxs2n18fHTw4MFMt4mPj8+0f3x8vHX97ba79cnMhAkTNG7cuByfAwAAAAAA+SnHoX/MmDF5UUeRMmLECJsnCBITE1W+fPkCrChrriVcdbV9UkGXAQAAAACFnmsJ14IuIdfkOPQXlNKlS8ve3l7nzp2zaT937px8fX0z3cbX1/ee/W//57lz5+Tn52fTp0GDBnetxcnJSU5OTvdzGgXGYmcxzeMpAAAAAIDsydHs/dKt9+jt7e3v+skrjo6OCgwM1Jo1a6xt6enpWrNmjYKCgjLdJigoyKa/JEVHR1v7V65cWb6+vjZ9EhMTtWXLlrvuEwAAAACAoiLHd/oXL15ss5yamqqdO3fq66+/zvP33IcMGaKIiAg1bNhQjRs31qRJk3T16lX16tVLktSjRw+VLVtWEyZMkCQNGjRIzZs310cffaT27dtr3rx5+u233/Tll19KkiwWiwYPHqx33nlHAQEBqly5st566y35+/sX2NwFAAAAAADklhyH/qeffjpDW+fOnVW7dm3Nnz9fffr0yZXCMtOlSxf98ccfGj16tOLj49WgQQOtXLnSOhHfiRMnZGf3fw8vNG3aVJGRkRo1apTefPNNBQQEKCoqSnXq1LH2ef3113X16lX17dtXly9f1j//+U+tXLlSzs7OeXYeAAAAAADkB4thGEZu7Oj3339XvXr1lJT095ssLru/jwgAAAAAQG7Ibg7N8Tv9mbl+/bomT56ssmXL5sbuAAAAAABALsjx4/0lS5aUxWKxLhuGoStXrsjV1VWzZ8/O1eIAAAAAAMD9y3Ho/+STT2xCv52dnby9vdWkSROVLFkyV4sDAAAAAAD3L8ehv2fPnnlQBgAAAAAAyG05fqd/5syZWrBgQYb2BQsW6Ouvv86VogAAAAAAwIPLceifMGGCSpcunaG9TJkyevfdd3OlKAAAAAAA8OByHPpPnDihypUrZ2ivWLGiTpw4kStFAQAAAACAB5fj0F+mTBnt3r07Q/uuXbvk5eWVK0UBAAAAAIAHl+PQHxYWpoEDB2rdunVKS0tTWlqa1q5dq0GDBqlr1655USMAAAAAALgPOZ69/+2339axY8fUqlUrOTjc2jw9PV09evTgnX4AAAAAAAoRi2EYxv1sePjwYcXGxsrFxUV169ZVxYoVc7u2IiMxMVEeHh5KSEiQu7t7QZcDAAAAADC57ObQHN/pvy0gIEABAQH3uzkAAAAAAMhjOX6nv1OnTnr//fcztH/wwQd69tlnc6UoAAAAAADw4HIc+jdu3Kh27dplaG/btq02btyYK0UBAAAAAIAHl+PQn5SUJEdHxwztxYoVU2JiYq4UBQAAAAAAHlyOQ3/dunU1f/78DO3z5s1TrVq1cqUoAAAAAADw4HI8kd9bb72lZ555RkePHlXLli0lSWvWrFFkZKQWLlyY6wUCAAAAAID7k+PQ36FDB0VFRendd9/VwoUL5eLiovr162vt2rUqVapUXtQIAAAAAADug8UwDONBdpCYmKi5c+dq+vTp2r59u9LS0nKrtiIju7+PCAAAAABAbshuDs3xO/23bdy4UREREfL399dHH32kli1b6tdff73f3QEAAAAAgFyWo8f74+PjNWvWLE2fPl2JiYl67rnnlJycrKioKCbxAwAAAACgkMn2nf4OHTqoevXq2r17tyZNmqQzZ87os88+y8vaAAAAAADAA8j2nf4VK1Zo4MCBevnllxUQEJCXNQEAAAAAgFyQ7Tv9P//8s65cuaLAwEA1adJEU6ZM0YULF/KyNgAAAAAA8ACyHfofeeQRffXVVzp79qz+9a9/ad68efL391d6erqio6N15cqVvKwTAAAAAADk0AP9ZN+hQ4c0ffp0ffvtt7p8+bJat26tJUuW5GZ9RQI/2QcAAAAAyE95/pN9klS9enV98MEHOnXqlObOnfsguwIAAAAAALnsge704xbu9AMAAAAA8lO+3OkHAAAAAACFF6EfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJlVkQv+lS5cUHh4ud3d3eXp6qk+fPkpKSrrnNjdu3FC/fv3k5eUlNzc3derUSefOnbOu37Vrl8LCwlS+fHm5uLioZs2a+vTTT/P6VAAAAAAAyBdFJvSHh4dr3759io6O1rJly7Rx40b17dv3ntu8+uqrWrp0qRYsWKANGzbozJkzeuaZZ6zrt2/frjJlymj27Nnat2+fRo4cqREjRmjKlCl5fToAAAAAAOQ5i2EYRkEXkZUDBw6oVq1a2rZtmxo2bChJWrlypdq1a6dTp07J398/wzYJCQny9vZWZGSkOnfuLEk6ePCgatasqZiYGD3yyCOZHqtfv346cOCA1q5dm+36EhMT5eHhoYSEBLm7u9/HGQIAAAAAkH3ZzaFF4k5/TEyMPD09rYFfkoKDg2VnZ6ctW7Zkus327duVmpqq4OBga1uNGjVUoUIFxcTE3PVYCQkJKlWq1D3rSU5OVmJios0HAAAAAIDCpkiE/vj4eJUpU8amzcHBQaVKlVJ8fPxdt3F0dJSnp6dNu4+Pz123+eWXXzR//vwsXxuYMGGCPDw8rJ/y5ctn/2QAAAAAAMgnBRr633jjDVkslnt+Dh48mC+17N27V08//bTGjBmjJ5544p59R4wYoYSEBOvn5MmT+VIjAAAAAAA54VCQBx86dKh69ux5zz5VqlSRr6+vzp8/b9N+8+ZNXbp0Sb6+vplu5+vrq5SUFF2+fNnmbv+5c+cybLN//361atVKffv21ahRo7Ks28nJSU5OTln2AwAAAACgIBVo6Pf29pa3t3eW/YKCgnT58mVt375dgYGBkqS1a9cqPT1dTZo0yXSbwMBAFStWTGvWrFGnTp0kSYcOHdKJEycUFBRk7bdv3z61bNlSERER+ve//50LZwUAAAAAQOFQJGbvl6S2bdvq3LlzmjZtmlJTU9WrVy81bNhQkZGRkqTTp0+rVatW+uabb9S4cWNJ0ssvv6zly5dr1qxZcnd314ABAyTdendfuvVIf8uWLRUSEqIPP/zQeix7e/tsfRlxG7P3AwAAAADyU3ZzaIHe6c+JOXPmqH///mrVqpXs7OzUqVMnTZ482bo+NTVVhw4d0rVr16xtn3zyibVvcnKyQkJC9MUXX1jXL1y4UH/88Ydmz56t2bNnW9srVqyoY8eO5ct5AQAAAACQV4rMnf7CjDv9AAAAAID8lN0cWiR+sg8AAAAAAOQcoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAATIrQDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAATIrQDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAATIrQDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAATIrQDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAATIrQDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAATIrQDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFKEfgAAAAAATIrQDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP0AAAAAAJgUoR8AAAAAAJMi9AMAAAAAYFJFJvRfunRJ4eHhcnd3l6enp/r06aOkpKR7bnPjxg3169dPXl5ecnNzU6dOnXTu3LlM+168eFHlypWTxWLR5cuX8+AMAAAAAADIX0Um9IeHh2vfvn2Kjo7WsmXLtHHjRvXt2/ee27z66qtaunSpFixYoA0bNujMmTN65plnMu3bp08f1atXLy9KBwAAAACgQFgMwzAKuoisHDhwQLVq1dK2bdvUsGFDSdLKlSvVrl07nTp1Sv7+/hm2SUhIkLe3tyIjI9W5c2dJ0sGDB1WzZk3FxMTokUcesfadOnWq5s+fr9GjR6tVq1b6888/5enpme36EhMT5eHhoYSEBLm7uz/YyQIAAAAAkIXs5tAicac/JiZGnp6e1sAvScHBwbKzs9OWLVsy3Wb79u1KTU1VcHCwta1GjRqqUKGCYmJirG379+/X+PHj9c0338jOLnuXIzk5WYmJiTYfAAAAAAAKmyIR+uPj41WmTBmbNgcHB5UqVUrx8fF33cbR0THDHXsfHx/rNsnJyQoLC9OHH36oChUqZLueCRMmyMPDw/opX758zk4IAAAAAIB8UKCh/4033pDFYrnn5+DBg3l2/BEjRqhmzZrq1q1bjrdLSEiwfk6ePJlHFQIAAAAAcP8cCvLgQ4cOVc+ePe/Zp0qVKvL19dX58+dt2m/evKlLly7J19c30+18fX2VkpKiy5cv29ztP3funHWbtWvXas+ePVq4cKEk6fb0BqVLl9bIkSM1bty4TPft5OQkJyen7JwiAAAAAAAFpkBDv7e3t7y9vbPsFxQUpMuXL2v79u0KDAyUdCuwp6enq0mTJpluExgYqGLFimnNmjXq1KmTJOnQoUM6ceKEgoKCJEnff/+9rl+/bt1m27Zt6t27tzZt2qSqVas+6OkBAAAAAFCgCjT0Z1fNmjXVpk0bvfjii5o2bZpSU1PVv39/de3a1Tpz/+nTp9WqVSt98803aty4sTw8PNSnTx8NGTJEpUqVkru7uwYMGKCgoCDrzP13BvsLFy5Yj5eT2fsBAAAAACiMikTol6Q5c+aof//+atWqlezs7NSpUydNnjzZuj41NVWHDh3StWvXrG2ffPKJtW9ycrJCQkL0xRdfFET5AAAAAADkO4tx+0V23Lfs/j4iAAAAAAC5Ibs5tEj8ZB8AAAAAAMg5Qj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACZF6AcAAAAAwKQI/QAAAAAAmBShHwAAAAAAk3Io6ALMwDAMSVJiYmIBVwIAAAAA+Du4nT9v59G7IfTngitXrkiSypcvX8CVAAAAAAD+Tq5cuSIPD4+7rrcYWX0tgCylp6frzJkzKlGihCwWS0GXk0FiYqLKly+vkydPyt3dvaDLATLFOEVhxxhFYccYRVHAOEVhV5TGqGEYunLlivz9/WVnd/c397nTnwvs7OxUrly5gi4jS+7u7oV+4AKMUxR2jFEUdoxRFAWMUxR2RWWM3usO/21M5AcAAAAAgEkR+gEAAAAAMClC/9+Ak5OTxowZIycnp4IuBbgrxikKO8YoCjvGKIoCxikKOzOOUSbyAwAAAADApLjTDwAAAACASRH6AQAAAAAwKUI/AAAAAAAmRegHAAAAAMCkCP1/A59//rkqVaokZ2dnNWnSRFu3bi3okvA3sXHjRnXo0EH+/v6yWCyKioqyWW8YhkaPHi0/Pz+5uLgoODhYhw8ftulz6dIlhYeHy93dXZ6enurTp4+SkpLy8SxgZhMmTFCjRo1UokQJlSlTRqGhoTp06JBNnxs3bqhfv37y8vKSm5ubOnXqpHPnztn0OXHihNq3by9XV1eVKVNGr732mm7evJmfpwKTmjp1qurVqyd3d3e5u7srKChIK1assK5nfKKwee+992SxWDR48GBrG+MUBW3s2LGyWCw2nxo1aljXm32MEvpNbv78+RoyZIjGjBmjHTt2qH79+goJCdH58+cLujT8DVy9elX169fX559/nun6Dz74QJMnT9a0adO0ZcsWFS9eXCEhIbpx44a1T3h4uPbt26fo6GgtW7ZMGzduVN++ffPrFGByGzZsUL9+/fTrr78qOjpaqampeuKJJ3T16lVrn1dffVVLly7VggULtGHDBp05c0bPPPOMdX1aWprat2+vlJQU/fLLL/r66681a9YsjR49uiBOCSZTrlw5vffee9q+fbt+++03tWzZUk8//bT27dsnifGJwmXbtm36z3/+o3r16tm0M05RGNSuXVtnz561fn7++WfrOtOPUQOm1rhxY6Nfv37W5bS0NMPf39+YMGFCAVaFvyNJxuLFi63L6enphq+vr/Hhhx9a2y5fvmw4OTkZc+fONQzDMPbv329IMrZt22bts2LFCsNisRinT5/Ot9rx93H+/HlDkrFhwwbDMG6NyWLFihkLFiyw9jlw4IAhyYiJiTEMwzCWL19u2NnZGfHx8dY+U6dONdzd3Y3k5OT8PQH8LZQsWdL473//y/hEoXLlyhUjICDAiI6ONpo3b24MGjTIMAz+HUXhMGbMGKN+/fqZrvs7jFHu9JtYSkqKtm/fruDgYGubnZ2dgoODFRMTU4CVAVJcXJzi4+NtxqeHh4eaNGliHZ8xMTHy9PRUw4YNrX2Cg4NlZ2enLVu25HvNML+EhARJUqlSpSRJ27dvV2pqqs04rVGjhipUqGAzTuvWrSsfHx9rn5CQECUmJlrvxgK5IS0tTfPmzdPVq1cVFBTE+ESh0q9fP7Vv395mPEr8O4rC4/Dhw/L391eVKlUUHh6uEydOSPp7jFGHgi4AeefChQtKS0uzGZyS5OPjo4MHDxZQVcAt8fHxkpTp+Ly9Lj4+XmXKlLFZ7+DgoFKlSln7ALklPT1dgwcP1qOPPqo6depIujUGHR0d5enpadP3znGa2Ti+vQ54UHv27FFQUJBu3LghNzc3LV68WLVq1VJsbCzjE4XCvHnztGPHDm3bti3DOv4dRWHQpEkTzZo1S9WrV9fZs2c1btw4NWvWTHv37v1bjFFCPwAAunWXau/evTbv+AGFQfXq1RUbG6uEhAQtXLhQERER2rBhQ0GXBUiSTp48qUGDBik6OlrOzs4FXQ6QqbZt21r/XK9ePTVp0kQVK1bUd999JxcXlwKsLH/weL+JlS5dWvb29hlmnjx37px8fX0LqCrglttj8F7j09fXN8Okkzdv3tSlS5cYw8hV/fv317Jly7Ru3TqVK1fO2u7r66uUlBRdvnzZpv+d4zSzcXx7HfCgHB0dVa1aNQUGBmrChAmqX7++Pv30U8YnCoXt27fr/Pnzevjhh+Xg4CAHBwdt2LBBkydPloODg3x8fBinKHQ8PT31j3/8Q0eOHPlb/FtK6DcxR0dHBQYGas2aNda29PR0rVmzRkFBQQVYGSBVrlxZvr6+NuMzMTFRW7ZssY7PoKAgXb58Wdu3b7f2Wbt2rdLT09WkSZN8rxnmYxiG+vfvr8WLF2vt2rWqXLmyzfrAwEAVK1bMZpweOnRIJ06csBmne/bssfmCKjo6Wu7u7qpVq1b+nAj+VtLT05WcnMz4RKHQqlUr7dmzR7GxsdZPw4YNFR4ebv0z4xSFTVJSko4ePSo/P7+/x7+lBT2TIPLWvHnzDCcnJ2PWrFnG/v37jb59+xqenp42M08CeeXKlSvGzp07jZ07dxqSjI8//tjYuXOncfz4ccMwDOO9994zPD09jR9++MHYvXu38fTTTxuVK1c2rl+/bt1HmzZtjIceesjYsmWL8fPPPxsBAQFGWFhYQZ0STObll182PDw8jPXr1xtnz561fq5du2bt89JLLxkVKlQw1q5da/z2229GUFCQERQUZF1/8+ZNo06dOsYTTzxhxMbGGitXrjS8vb2NESNGFMQpwWTeeOMNY8OGDUZcXJyxe/du44033jAsFovx008/GYbB+ETh9NfZ+w2DcYqCN3ToUGP9+vVGXFycsXnzZiM4ONgoXbq0cf78ecMwzD9GCf1/A5999plRoUIFw9HR0WjcuLHx66+/FnRJ+JtYt26dISnDJyIiwjCMWz/b99Zbbxk+Pj6Gk5OT0apVK+PQoUM2+7h48aIRFhZmuLm5Ge7u7kavXr2MK1euFMDZwIwyG5+SjJkzZ1r7XL9+3XjllVeMkiVLGq6urkbHjh2Ns2fP2uzn2LFjRtu2bQ0XFxejdOnSxtChQ43U1NR8PhuYUe/evY2KFSsajo6Ohre3t9GqVStr4DcMxicKpztDP+MUBa1Lly6Gn5+f4ejoaJQtW9bo0qWLceTIEet6s49Ri2EYRsE8YwAAAAAAAPIS7/QDAAAAAGBShH4AAAAAAEyK0A8AAAAAgEkR+gEAAAAAMClCPwAAAAAAJkXoBwAAAADApAj9AAAAAACYFKEfAAAAAACTIvQDAIBCz2KxKCoqqqDLAACgyCH0AwCAe+rZs6csFkuGT5s2bQq6NAAAkAWHgi4AAAAUfm3atNHMmTNt2pycnAqoGgAAkF3c6QcAAFlycnKSr6+vzadkyZKSbj16P3XqVLVt21YuLi6qUqWKFi5caLP9nj171LJlS7m4uMjLy0t9+/ZVUlKSTZ8ZM2aodu3acnJykp+fn/r372+z/sKFC+rYsaNcXV0VEBCgJUuWWNf9+eefCg8Pl7e3t1xcXBQQEJDhSwoAAP6OCP0AAOCBvfXWW+rUqZN27dql8PBwde3aVQcOHJAkXb16VSEhISpZsqS2bdumBQsWaPXq1TahfurUqerXr5/69u2rPXv2aMmSJapWrZrNMcaNG6fnnntOu3fvVrt27RQeHq5Lly5Zj79//36tWLFCBw4c0NSpU1W6dOn8uwAAABRSFsMwjIIuAgAAFF49e/bU7Nmz5ezsbNP+5ptv6s0335TFYtFLL72kqVOnWtc98sgjevjhh/XFF1/oq6++0vDhw3Xy5EkVL15ckrR8+XJ16NBBZ86ckY+Pj8qWLatevXrpnXfeybQGi8WiUaNG6e2335Z064sENzc3rVixQm3atNFTTz2l0qVLa8aMGXl0FQAAKJp4px8AAGTp8ccftwn1klSqVCnrn4OCgmzWBQUFKTY2VpJ04MAB1a9f3xr4JenRRx9Venq6Dh06JIvFojNnzqhVq1b3rKFevXrWPxcvXlzu7u46f/68JOnll19Wp06dtGPHDj3xxBMKDQ1V06ZN7+tcAQAwE0I/AADIUvHixTM8bp9bXFxcstWvWLFiNssWi0Xp6emSpLZt2+r48eNavny5oqOj1apVK/Xr108TJ07M9XoBAChKeKcfAAA8sF9//TXDcs2aNSVJNWvW1K5du3T16lXr+s2bN8vOzk7Vq1dXiRIlVKlSJa1Zs+aBavD29lZERIRmz56tSZMm6csvv3yg/QEAYAbc6QcAAFlKTk5WfHy8TZuDg4N1srwFCxaoYcOG+uc//6k5c+Zo69atmj59uiQpPDxcY8aMUUREhMaOHas//vhDAwYMUPfu3eXj4yNJGjt2rF566SWVKVNGbdu21ZUrV7R582YNGDAgW/WNHj1agYGBql27tpKTk7Vs2TLrlw4AAPydEfoBAECWVq5cKT8/P5u26tWr6+DBg5Juzaw/b948vfLKK/Lz89PcuXNVq1YtSZKrq6tWrVqlQYMGqVGjRnJ1dVWnTp308ccfW/cVERGhGzdu6JNPPtGwYcNUunRpde7cOdv1OTo6asSIETp27JhcXFzUrFkzzZs3LxfOHACAoo3Z+wEAwAOxWCxavHixQkNDC7oUAABwB97pBwAAAADApAj9AAAAAACYFO/0AwCAB8KbggAAFF7c6QcAAAAAwKQI/QAAAAAAmBShHwAAAAAAkyL0AwAAAABgUoR+AAAAAABMitAPAAAAAIBJEfoBAAAAADApQj8AAAAAACb1/wDBgLOcPyLiQwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, len(training_accuracy) + 1), training_accuracy, label='Training Accuracy', color='blue')\n",
    "plt.plot(range(1, len(validation_accuracy) + 1), validation_accuracy, label='Validation Accuracy',  color='orange')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengambil loss dari setiap epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = trained_data.history['loss']\n",
    "validation_loss = trained_data.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat grafik untuk loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(1, len(training_loss) + 1), training_loss, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, len(validation_loss) + 1), validation_loss, label='Validation Loss', color='orange')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c2-21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
